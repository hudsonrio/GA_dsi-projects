{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import psycopg2 as psy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from matplotlib import *\n",
    "import sys\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection)\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold, permutation_test_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "import matplotlib.patheffects as path_effects\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# some helper functions\n",
    "\n",
    "def pop_year_clean(year):\n",
    "    year = str(year)\n",
    "    year = year.replace(\" \", \"\")\n",
    "    try:\n",
    "        year = int(year)\n",
    "    except:\n",
    "        year = year\n",
    "    return year\n",
    "\n",
    "\n",
    "def state_fix(code):\n",
    "    code = str(code)\n",
    "    if len(code) == 1:\n",
    "        code = \"0\" + code\n",
    "    return code\n",
    "\n",
    "def county_fix(code):\n",
    "    code = str(code)\n",
    "    if len(code) == 1:\n",
    "        code = \"00\" + code\n",
    "    elif len(code) ==2:\n",
    "        code = \"0\" + code\n",
    "    return code\n",
    "\n",
    "def col_includer(year, col):\n",
    "    if year in col:\n",
    "        return col\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#used for growth data\n",
    "def growth_convert(df, year):\n",
    "    final_cols = []\n",
    "    df_cols = df.columns\n",
    "    last_year = str(year - 1)\n",
    "    year = str(year)\n",
    "    df_cols_this = df_cols.map(lambda x: col_includer(year, x))\n",
    "    df_cols_last = df_cols.map(lambda x: col_includer(last_year, x))\n",
    "\n",
    "    #cleaning this year col\n",
    "    df_cols_this = pd.DataFrame(df_cols_this)\n",
    "    df_cols_this = df_cols_this[df_cols_this[0]!=0]\n",
    "    final_cols.append(df_cols_this.iloc[0,0])\n",
    "    #pulling last year's columns\n",
    "    df_cols_last = pd.DataFrame(df_cols_last)\n",
    "    df_cols_last = df_cols_last[df_cols_last[0]!=0]\n",
    "    final_cols.append(df_cols_last.iloc[1,0])\n",
    "    final_cols.append(df_cols_last.iloc[12,0])\n",
    "    final_cols.append(df_cols_last.iloc[13,0])\n",
    "    final_cols.append(df_cols_last.iloc[14,0])\n",
    "    final_cols.append('STATE')\n",
    "    final_cols.append('COUNTY')\n",
    "    df = df[final_cols]\n",
    "    df['STATE'] = df['STATE'].apply(lambda x: state_fix(x))\n",
    "    df['COUNTY'] = df['COUNTY'].apply(lambda x: county_fix(x))\n",
    "    df['string_county_code'] = df['STATE'] + df['COUNTY']\n",
    "    df['county_code'] = df['string_county_code'].apply(lambda x: int(x))\n",
    "    df['year'] = int(year)\n",
    "    df.columns = ['population_est', 'net_pop_change_raw','natural_pop_growth_rate', 'intl_migrate_rate', 'dom_migrate_rate', 'state_num', 'county_num', 'string_county_code', 'county_code', 'year']\n",
    "    return df\n",
    "\n",
    "def yearify(year):\n",
    "    year = float(year)\n",
    "    try:\n",
    "        return int(year)\n",
    "    except:\n",
    "        return year\n",
    "\n",
    "def interator(example):\n",
    "    try:\n",
    "        example = str(example).split('.')[0]\n",
    "        return int(example)\n",
    "    except:\n",
    "        return example\n",
    "\n",
    "def death_clean(death):\n",
    "    try:\n",
    "        return float(death)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def state_extract(county):\n",
    "    county = str(county)\n",
    "    try:\n",
    "        county = county.split(\",\")[1]\n",
    "        county = county.replace(\" \", \"\")\n",
    "        return county\n",
    "    except:\n",
    "        if len(county) == 2:\n",
    "            return county\n",
    "        else:\n",
    "            return \"ERROR\"\n",
    "\n",
    "\n",
    "def floater(df):\n",
    "    for col in df:\n",
    "        try:\n",
    "            col = col.apply(lambda x: float(x))\n",
    "        except:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "def dict_applier(df):\n",
    "    dict_ex = defaultdict(lambda: 0)\n",
    "    for i in range(len(df)):\n",
    "        dict_ex[df['county_code'][i]] +=1\n",
    "        df['count_county_code'][i] = dict_ex[df['county_code'][i]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', labels=['Decreasing','Increasing'], cmap=plt.cm.Blues):\n",
    "\n",
    "   plt.figure(figsize=(7,7))\n",
    "   plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "   tick_marks = np.arange(2)\n",
    "   plt.xticks(tick_marks, labels)\n",
    "   plt.yticks(tick_marks, labels)\n",
    "\n",
    "   plt.title(title)\n",
    "   plt.ylabel('True label')\n",
    "   plt.xlabel('Predicted label')\n",
    "   plt.colorbar()\n",
    "   plt.tight_layout()\n",
    "\n",
    "   width, height = cm.shape\n",
    "\n",
    "   for x in xrange(width):\n",
    "       for y in xrange(height):\n",
    "           plt.annotate(str(cm[x][y]), xy=(y, x),\n",
    "                       horizontalalignment='center',\n",
    "                       verticalalignment='center',\n",
    "                       color = 'white',\n",
    "                       fontsize=18).set_path_effects([path_effects.Stroke(linewidth=1, foreground='black'),\n",
    "                                                      path_effects.Normal()]) #The last line here adds a text outline\n",
    "\n",
    "\n",
    "\n",
    "#reading in unemployment data\n",
    "\n",
    "unemp_03_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/unemp_tot_03_14.csv')\n",
    "unemp_03_14 = unemp_03_14.iloc[:,1:]\n",
    "\n",
    "#reading in poverty data\n",
    "pov_county_year_03_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pov_county_year_03_14.csv')\n",
    "pov_county_year_03_14 = pov_county_year_03_14.iloc[:,1:]\n",
    "\n",
    "\n",
    "#  removed from model, only used for name, state conversion\n",
    "cdc_03_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/cdc_03_14.csv')\n",
    "cdc_03_14 = cdc_03_14.iloc[:,2:]\n",
    "\n",
    "name_codes = cdc_03_14.loc[:,['county_code', 'County']] #grabbing this\n",
    "name_codes['county_code'] = name_codes['county_code'].apply(lambda x: int(x))\n",
    "name_codes.index=name_codes['county_code']\n",
    "name_codes = name_codes.iloc[:,1:]\n",
    "name_codes = name_codes.to_dict()\n",
    "name_codes = name_codes['County']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45063,\n",
       " 45077,\n",
       " 45079,\n",
       " 45083,\n",
       " 45091,\n",
       " 41005,\n",
       " 4025,\n",
       " 47157,\n",
       " 41017,\n",
       " 47163,\n",
       " 41029,\n",
       " 41033,\n",
       " 41035,\n",
       " 41039,\n",
       " 41043,\n",
       " 41047,\n",
       " 41051,\n",
       " 39017,\n",
       " 41067,\n",
       " 39023,\n",
       " 39025,\n",
       " 46099,\n",
       " 39035,\n",
       " 39049,\n",
       " 39057,\n",
       " 39061,\n",
       " 37019,\n",
       " 37021,\n",
       " 34035,\n",
       " 39081,\n",
       " 37035,\n",
       " 39085,\n",
       " 39093,\n",
       " 39095,\n",
       " 35001,\n",
       " 37051,\n",
       " 37057,\n",
       " 17099,\n",
       " 35013,\n",
       " 39113,\n",
       " 37067,\n",
       " 37071,\n",
       " 35031,\n",
       " 37081,\n",
       " 35039,\n",
       " 37089,\n",
       " 35043,\n",
       " 35045,\n",
       " 35049,\n",
       " 39151,\n",
       " 39153,\n",
       " 39155,\n",
       " 33013,\n",
       " 33015,\n",
       " 33017,\n",
       " 39165,\n",
       " 37119,\n",
       " 37129,\n",
       " 47149,\n",
       " 37151,\n",
       " 37159,\n",
       " 37183,\n",
       " 12001,\n",
       " 37193,\n",
       " 31055,\n",
       " 51550,\n",
       " 17201,\n",
       " 42043,\n",
       " 17197,\n",
       " 29047,\n",
       " 27003,\n",
       " 31109,\n",
       " 1089,\n",
       " 29077,\n",
       " 27037,\n",
       " 28059,\n",
       " 29095,\n",
       " 25001,\n",
       " 29099,\n",
       " 27053,\n",
       " 25009,\n",
       " 54003,\n",
       " 25013,\n",
       " 48201,\n",
       " 25017,\n",
       " 25021,\n",
       " 25023,\n",
       " 25025,\n",
       " 47179,\n",
       " 23005,\n",
       " 12019,\n",
       " 13051,\n",
       " 23019,\n",
       " 27123,\n",
       " 23031,\n",
       " 51710,\n",
       " 29183,\n",
       " 27137,\n",
       " 25003,\n",
       " 29189,\n",
       " 42071,\n",
       " 25005,\n",
       " 21015,\n",
       " 21037,\n",
       " 42077,\n",
       " 51760,\n",
       " 51770,\n",
       " 21059,\n",
       " 21067,\n",
       " 21071,\n",
       " 51810,\n",
       " 10001,\n",
       " 10003,\n",
       " 53077,\n",
       " 21111,\n",
       " 17019,\n",
       " 21117,\n",
       " 10005,\n",
       " 17031,\n",
       " 17043,\n",
       " 15003,\n",
       " 8001,\n",
       " 45045,\n",
       " 19113,\n",
       " 42101,\n",
       " 17089,\n",
       " 17097,\n",
       " 21195,\n",
       " 19153,\n",
       " 56021,\n",
       " 17111,\n",
       " 17113,\n",
       " 19163,\n",
       " 17119,\n",
       " 55079,\n",
       " 21235,\n",
       " 54005,\n",
       " 17143,\n",
       " 55059,\n",
       " 11001,\n",
       " 54011,\n",
       " 13057,\n",
       " 13067,\n",
       " 17167,\n",
       " 8005,\n",
       " 54039,\n",
       " 13089,\n",
       " 54055,\n",
       " 9001,\n",
       " 9003,\n",
       " 9005,\n",
       " 9007,\n",
       " 9009,\n",
       " 9011,\n",
       " 9015,\n",
       " 54081,\n",
       " 29510,\n",
       " 26081,\n",
       " 13135,\n",
       " 13139,\n",
       " 54109,\n",
       " 55101,\n",
       " 6099,\n",
       " 13121,\n",
       " 5007,\n",
       " 48029,\n",
       " 12101,\n",
       " 48039,\n",
       " 12103,\n",
       " 15009,\n",
       " 5051,\n",
       " 48061,\n",
       " 12109,\n",
       " 48085,\n",
       " 41019,\n",
       " 44003,\n",
       " 44007,\n",
       " 44009,\n",
       " 1003,\n",
       " 48113,\n",
       " 48121,\n",
       " 5119,\n",
       " 5131,\n",
       " 48141,\n",
       " 42003,\n",
       " 42007,\n",
       " 42011,\n",
       " 48157,\n",
       " 42017,\n",
       " 42021,\n",
       " 48167,\n",
       " 42029,\n",
       " 1073,\n",
       " 8031,\n",
       " 42045,\n",
       " 42049,\n",
       " 42051,\n",
       " 1097,\n",
       " 42069,\n",
       " 48215,\n",
       " 40027,\n",
       " 1117,\n",
       " 42079,\n",
       " 39099,\n",
       " 1125,\n",
       " 1127,\n",
       " 42091,\n",
       " 42095,\n",
       " 48245,\n",
       " 42107,\n",
       " 42125,\n",
       " 42129,\n",
       " 42133,\n",
       " 36001,\n",
       " 36005,\n",
       " 40109,\n",
       " 49011,\n",
       " 48309,\n",
       " 36027,\n",
       " 36029,\n",
       " 36047,\n",
       " 34001,\n",
       " 26147,\n",
       " 34005,\n",
       " 34007,\n",
       " 36059,\n",
       " 34013,\n",
       " 34015,\n",
       " 36065,\n",
       " 48355,\n",
       " 34021,\n",
       " 36071,\n",
       " 34025,\n",
       " 34027,\n",
       " 34029,\n",
       " 34031,\n",
       " 36081,\n",
       " 34003,\n",
       " 36085,\n",
       " 34039,\n",
       " 12113,\n",
       " 32003,\n",
       " 36103,\n",
       " 36055,\n",
       " 36119,\n",
       " 32031,\n",
       " 47009,\n",
       " 40143,\n",
       " 36061,\n",
       " 48439,\n",
       " 36063,\n",
       " 4005,\n",
       " 48453,\n",
       " 34017,\n",
       " 8077,\n",
       " 36067,\n",
       " 48479,\n",
       " 48485,\n",
       " 48491,\n",
       " 4017,\n",
       " 4019,\n",
       " 39145,\n",
       " 28047,\n",
       " 49049,\n",
       " 36079,\n",
       " 30111,\n",
       " 26021,\n",
       " 26025,\n",
       " 6045,\n",
       " 33011,\n",
       " 34037,\n",
       " 26049,\n",
       " 24003,\n",
       " 24005,\n",
       " 49057,\n",
       " 48375,\n",
       " 24013,\n",
       " 26065,\n",
       " 34023,\n",
       " 24025,\n",
       " 26075,\n",
       " 26077,\n",
       " 24031,\n",
       " 24033,\n",
       " 24043,\n",
       " 26093,\n",
       " 26099,\n",
       " 22017,\n",
       " 26115,\n",
       " 26121,\n",
       " 26125,\n",
       " 6061,\n",
       " 22033,\n",
       " 26139,\n",
       " 26145,\n",
       " 22051,\n",
       " 22055,\n",
       " 22063,\n",
       " 26161,\n",
       " 26163,\n",
       " 22071,\n",
       " 53041,\n",
       " 22087,\n",
       " 18003,\n",
       " 22103,\n",
       " 8101,\n",
       " 6075,\n",
       " 6077,\n",
       " 20091,\n",
       " 16001,\n",
       " 16019,\n",
       " 48339,\n",
       " 18089,\n",
       " 18095,\n",
       " 18097,\n",
       " 18105,\n",
       " 20173,\n",
       " 18127,\n",
       " 20177,\n",
       " 6095,\n",
       " 18141,\n",
       " 55009,\n",
       " 12005,\n",
       " 12009,\n",
       " 12011,\n",
       " 12015,\n",
       " 12017,\n",
       " 18163,\n",
       " 12021,\n",
       " 55025,\n",
       " 12031,\n",
       " 12033,\n",
       " 53005,\n",
       " 22019,\n",
       " 53009,\n",
       " 53011,\n",
       " 12053,\n",
       " 53015,\n",
       " 12057,\n",
       " 12061,\n",
       " 6107,\n",
       " 12069,\n",
       " 12071,\n",
       " 53033,\n",
       " 53035,\n",
       " 12081,\n",
       " 12083,\n",
       " 12085,\n",
       " 12086,\n",
       " 12087,\n",
       " 12091,\n",
       " 53053,\n",
       " 12095,\n",
       " 55105,\n",
       " 12099,\n",
       " 53061,\n",
       " 53063,\n",
       " 12105,\n",
       " 53067,\n",
       " 8013,\n",
       " 12111,\n",
       " 53073,\n",
       " 12115,\n",
       " 12117,\n",
       " 55133,\n",
       " 12127,\n",
       " 55139,\n",
       " 8041,\n",
       " 37097,\n",
       " 6001,\n",
       " 51059,\n",
       " 6007,\n",
       " 8059,\n",
       " 6013,\n",
       " 6017,\n",
       " 6019,\n",
       " 8069,\n",
       " 6023,\n",
       " 6025,\n",
       " 49035,\n",
       " 6029,\n",
       " 51087,\n",
       " 6033,\n",
       " 6037,\n",
       " 6041,\n",
       " 49053,\n",
       " 6047,\n",
       " 4001,\n",
       " 6053,\n",
       " 6059,\n",
       " 4013,\n",
       " 4015,\n",
       " 6065,\n",
       " 6067,\n",
       " 4021,\n",
       " 6071,\n",
       " 6073,\n",
       " 8123,\n",
       " 47037,\n",
       " 24510,\n",
       " 6079,\n",
       " 6081,\n",
       " 6083,\n",
       " 6085,\n",
       " 6087,\n",
       " 6089,\n",
       " 45007,\n",
       " 6097,\n",
       " 47059,\n",
       " 47065,\n",
       " 45019,\n",
       " 6111,\n",
       " 6113,\n",
       " 2020,\n",
       " 25027,\n",
       " 29187,\n",
       " 47093,\n",
       " 45051]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_codes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Douglas County, NE'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_codes[31055]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HudsonCavanagh/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/HudsonCavanagh/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/HudsonCavanagh/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/HudsonCavanagh/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/HudsonCavanagh/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/HudsonCavanagh/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>county_code</th>\n",
       "      <th>deaths_raw</th>\n",
       "      <th>cdc_population</th>\n",
       "      <th>death_rate_100k_cdc</th>\n",
       "      <th>AGEGRP</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>RACE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>TOT_POP</th>\n",
       "      <th>...</th>\n",
       "      <th>drug_death_rate_100k_cdc</th>\n",
       "      <th>drug_death_rate_100k</th>\n",
       "      <th>perc_pop_employed</th>\n",
       "      <th>unemp_rate</th>\n",
       "      <th>perc_pop_working</th>\n",
       "      <th>pov_rate</th>\n",
       "      <th>pov_rate_youth</th>\n",
       "      <th>pop_change_rate</th>\n",
       "      <th>drug_death_rate_100k_diff_cdc_pop</th>\n",
       "      <th>constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>103025.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32319.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>10773.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.677020</td>\n",
       "      <td>10.704867</td>\n",
       "      <td>0.421879</td>\n",
       "      <td>0.065470</td>\n",
       "      <td>0.451434</td>\n",
       "      <td>0.156038</td>\n",
       "      <td>0.055695</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>-0.027847</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>2005.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>103174.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32319.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>10773.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.600074</td>\n",
       "      <td>12.665998</td>\n",
       "      <td>0.433976</td>\n",
       "      <td>0.047474</td>\n",
       "      <td>0.455606</td>\n",
       "      <td>0.155646</td>\n",
       "      <td>0.051901</td>\n",
       "      <td>-0.000682</td>\n",
       "      <td>-0.065924</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>2006.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>103528.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32319.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>10773.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.659223</td>\n",
       "      <td>9.720913</td>\n",
       "      <td>0.433028</td>\n",
       "      <td>0.042454</td>\n",
       "      <td>0.452227</td>\n",
       "      <td>0.164993</td>\n",
       "      <td>0.055380</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>-0.061690</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>2007.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>103893.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32319.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>10773.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.475403</td>\n",
       "      <td>13.566680</td>\n",
       "      <td>0.420867</td>\n",
       "      <td>0.044612</td>\n",
       "      <td>0.440520</td>\n",
       "      <td>0.170630</td>\n",
       "      <td>0.055449</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>-0.091278</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>104206.0</td>\n",
       "      <td>19.19275</td>\n",
       "      <td>32319.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>10773.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.192753</td>\n",
       "      <td>19.335815</td>\n",
       "      <td>0.415672</td>\n",
       "      <td>0.059746</td>\n",
       "      <td>0.442084</td>\n",
       "      <td>0.163610</td>\n",
       "      <td>0.054227</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>-0.143062</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>2009.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>104239.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32319.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>10773.0</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.349341</td>\n",
       "      <td>15.474187</td>\n",
       "      <td>0.385288</td>\n",
       "      <td>0.114573</td>\n",
       "      <td>0.435144</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0.060891</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>-0.124846</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>104430.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>208884.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.490951</td>\n",
       "      <td>11.588605</td>\n",
       "      <td>0.378493</td>\n",
       "      <td>0.109250</td>\n",
       "      <td>0.424915</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.065447</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.097654</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5608</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>104303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>208472.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.381178</td>\n",
       "      <td>14.390422</td>\n",
       "      <td>0.386028</td>\n",
       "      <td>0.100264</td>\n",
       "      <td>0.429046</td>\n",
       "      <td>0.205553</td>\n",
       "      <td>0.068537</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6447</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>104392.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>208470.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.579278</td>\n",
       "      <td>9.593707</td>\n",
       "      <td>0.389121</td>\n",
       "      <td>0.082477</td>\n",
       "      <td>0.424099</td>\n",
       "      <td>0.206341</td>\n",
       "      <td>0.069746</td>\n",
       "      <td>-0.001976</td>\n",
       "      <td>-0.014428</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7281</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>103931.0</td>\n",
       "      <td>20.20571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>207704.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.205713</td>\n",
       "      <td>20.221084</td>\n",
       "      <td>0.389063</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>0.421070</td>\n",
       "      <td>0.191985</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.015370</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8131</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>103531.0</td>\n",
       "      <td>20.28378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206904.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.283780</td>\n",
       "      <td>20.299269</td>\n",
       "      <td>0.392830</td>\n",
       "      <td>0.070684</td>\n",
       "      <td>0.422708</td>\n",
       "      <td>0.186473</td>\n",
       "      <td>0.063856</td>\n",
       "      <td>-0.003702</td>\n",
       "      <td>-0.015489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  county_code  deaths_raw  cdc_population  death_rate_100k_cdc  \\\n",
       "1     2003.0       1055.0        11.0        103025.0                  NaN   \n",
       "1169  2005.0       1055.0        13.0        103174.0                  NaN   \n",
       "1814  2006.0       1055.0        10.0        103528.0                  NaN   \n",
       "2532  2007.0       1055.0        14.0        103893.0                  NaN   \n",
       "3262  2008.0       1055.0        20.0        104206.0             19.19275   \n",
       "4042  2009.0       1055.0        16.0        104239.0                  NaN   \n",
       "4798  2010.0       1055.0        12.0        104430.0                  NaN   \n",
       "5608  2011.0       1055.0        15.0        104303.0                  NaN   \n",
       "6447  2012.0       1055.0        10.0        104392.0                  NaN   \n",
       "7281  2013.0       1055.0        21.0        103931.0             20.20571   \n",
       "8131  2014.0       1055.0        21.0        103531.0             20.28378   \n",
       "\n",
       "       AGEGRP  ORIGIN     RACE     SEX   TOT_POP    ...     \\\n",
       "1     32319.0  3591.0  10773.0  3591.0       NaN    ...      \n",
       "1169  32319.0  3591.0  10773.0  3591.0       NaN    ...      \n",
       "1814  32319.0  3591.0  10773.0  3591.0       NaN    ...      \n",
       "2532  32319.0  3591.0  10773.0  3591.0       NaN    ...      \n",
       "3262  32319.0  3591.0  10773.0  3591.0       NaN    ...      \n",
       "4042  32319.0  3591.0  10773.0  3591.0       NaN    ...      \n",
       "4798      NaN     NaN      NaN     NaN  208884.0    ...      \n",
       "5608      NaN     NaN      NaN     NaN  208472.0    ...      \n",
       "6447      NaN     NaN      NaN     NaN  208470.0    ...      \n",
       "7281      NaN     NaN      NaN     NaN  207704.0    ...      \n",
       "8131      NaN     NaN      NaN     NaN  206904.0    ...      \n",
       "\n",
       "      drug_death_rate_100k_cdc  drug_death_rate_100k  perc_pop_employed  \\\n",
       "1                    10.677020             10.704867           0.421879   \n",
       "1169                 12.600074             12.665998           0.433976   \n",
       "1814                  9.659223              9.720913           0.433028   \n",
       "2532                 13.475403             13.566680           0.420867   \n",
       "3262                 19.192753             19.335815           0.415672   \n",
       "4042                 15.349341             15.474187           0.385288   \n",
       "4798                 11.490951             11.588605           0.378493   \n",
       "5608                 14.381178             14.390422           0.386028   \n",
       "6447                  9.579278              9.593707           0.389121   \n",
       "7281                 20.205713             20.221084           0.389063   \n",
       "8131                 20.283780             20.299269           0.392830   \n",
       "\n",
       "      unemp_rate  perc_pop_working  pov_rate  pov_rate_youth  pop_change_rate  \\\n",
       "1       0.065470          0.451434  0.156038        0.055695        -0.000749   \n",
       "1169    0.047474          0.455606  0.155646        0.051901        -0.000682   \n",
       "1814    0.042454          0.452227  0.164993        0.055380        -0.000486   \n",
       "2532    0.044612          0.440520  0.170630        0.055449         0.002268   \n",
       "3262    0.059746          0.442084  0.163610        0.054227         0.003123   \n",
       "4042    0.114573          0.435144  0.168930        0.060891         0.002331   \n",
       "4798    0.109250          0.424915  0.187900        0.065447        -0.000357   \n",
       "5608    0.100264          0.429046  0.205553        0.068537         0.001458   \n",
       "6447    0.082477          0.424099  0.206341        0.069746        -0.001976   \n",
       "7281    0.076014          0.421070  0.191985        0.061453        -0.000010   \n",
       "8131    0.070684          0.422708  0.186473        0.063856        -0.003702   \n",
       "\n",
       "      drug_death_rate_100k_diff_cdc_pop  constant  \n",
       "1                             -0.027847       1.0  \n",
       "1169                          -0.065924       1.0  \n",
       "1814                          -0.061690       1.0  \n",
       "2532                          -0.091278       1.0  \n",
       "3262                          -0.143062       1.0  \n",
       "4042                          -0.124846       1.0  \n",
       "4798                          -0.097654       1.0  \n",
       "5608                          -0.009244       1.0  \n",
       "6447                          -0.014428       1.0  \n",
       "7281                          -0.015370       1.0  \n",
       "8131                          -0.015489       1.0  \n",
       "\n",
       "[11 rows x 67 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#these files were large, had to be groupby'd in seperate scripts on EC2 before reading in\n",
    "pop_03 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pop_03_output.csv')\n",
    "pop_04 =pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pop_04_output.csv')\n",
    "pop_05 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pop_05_output.csv')\n",
    "pop_06 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pop_06_output.csv')\n",
    "pop_07 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pop_07_output.csv')\n",
    "pop_08 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pop_08_output.csv')\n",
    "pop_09 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pop_09_output.csv')\n",
    "pop_10_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pop_10_15_output.csv')\n",
    "\n",
    "#standardizing formatting to merge\n",
    "pop_10_14 = pop_10_14.loc[:,['year', 'county_code', 'pop_sub_15', 'pop_15-34', 'pop_35-54', 'pop_55+','pop_black','pop_white','pop_hisp','pop_asian', 'pop_male', 'population', 'TOT_POP']]\n",
    "\n",
    "#merged population data\n",
    "pop_03_14 = pd.concat([pop_03, pop_04, pop_05, pop_06, pop_07, pop_08, pop_09, pop_10_14], axis=0) #this works 1) need to make floats 2) need more data\n",
    "pop_03_14['year'] = pop_03_14['year'].apply(lambda x: pop_year_clean(x)) #fixing integer conversion issue\n",
    "\n",
    "\n",
    "### Making caluclations with pop DataFrame\n",
    "\n",
    "pop_03_14['pop_total_age'] = pop_03_14['pop_sub_15'] + pop_03_14['pop_15-34'] + pop_03_14['pop_35-54'] + pop_03_14['pop_55+']\n",
    "pop_03_14['pop_sub_15_prop'] = pop_03_14['pop_sub_15']/pop_03_14['pop_total_age']\n",
    "pop_03_14['pop_15-34_prop'] = pop_03_14['pop_sub_15']/pop_03_14['pop_total_age']\n",
    "pop_03_14['pop_35-54_prop'] = pop_03_14['pop_sub_15']/pop_03_14['pop_total_age']\n",
    "pop_03_14['pop_55+_prop'] = pop_03_14['pop_sub_15']/pop_03_14['pop_total_age']\n",
    "pop_03_14['pop_big_3_races'] = pop_03_14['pop_black'] + pop_03_14['pop_white'] + pop_03_14['pop_asian']\n",
    "pop_03_14['pop_asian_prop'] = pop_03_14['pop_asian']/pop_03_14['pop_big_3_races']\n",
    "pop_03_14['pop_white_prop'] = pop_03_14['pop_white']/pop_03_14['pop_big_3_races']\n",
    "pop_03_14['pop_black_prop'] = pop_03_14['pop_black']/pop_03_14['pop_big_3_races']\n",
    "# pop_03_14['pop_hisp_prop'] = pop_03_14['pop_hisp']/pop_03_14['pop_big_3_races'] #don't have good relative comparison\n",
    "#excluded hispanic status is not mutually exclusive, no denominator to compare to &\n",
    "pop_03_14['pop_male_prop'] = pop_03_14['pop_male'] / pop_03_14['pop_total_age']\n",
    "\n",
    "\n",
    "# ## Merging Fentanyl and Naxolone data here\n",
    "fent_13_14 = pd.read_csv('/Users/HudsonCavanagh/Dropbox/Capstone/non_cdc_data/drug_seizures_year.csv')\n",
    "fent_13_14 = fent_13_14.iloc[:,1:]\n",
    "fent_13_14.columns = ['year', 'national_opiate_seizures', 'national_fentanyl_seizures']\n",
    "\n",
    "naxo = pd.read_csv('/Users/HudsonCavanagh/Dropbox/Capstone/non_cdc_data/naxo_years_clean.csv')\n",
    "naxo.columns = ['year', 'state', 'naxo_crim', 'naxo_civ', 'naxo_third']\n",
    "\n",
    "#### Merge Population, Population Growth Below\n",
    "\n",
    "pop_grow_10_14 = pd.read_csv('/Users/HudsonCavanagh/Dropbox/Capstone/non_cdc_data/pop_dense_10_14.csv') #need to find where this file actually is\n",
    "pop_grow_00_10 = pd.read_csv('/Users/HudsonCavanagh/Dropbox/Capstone/non_cdc_data/pop_grow_00_10.csv') #need to find where this file actually is\n",
    "\n",
    "\n",
    "#for 2011 population growth, need to do it in 2 parts b/c different CSV layout\n",
    "#first get 2011 pop estimate\n",
    "\n",
    "pop_grow_11 = pop_grow_10_14\n",
    "pop_grow_11['STATE'] = pop_grow_11['STATE'].apply(lambda x: state_fix(x))\n",
    "pop_grow_11['COUNTY'] = pop_grow_11['COUNTY'].apply(lambda x: county_fix(x))\n",
    "pop_grow_11['string_county_code'] = pop_grow_11['STATE'] + pop_grow_11['COUNTY']\n",
    "pop_grow_11['county_code'] = pop_grow_11['string_county_code'].apply(lambda x: int(x))\n",
    "pop_grow_11['year'] = 2011\n",
    "pop_grow_11 = pop_grow_11[['POPESTIMATE2011', 'county_code', 'year']]\n",
    "pop_grow_11.columns = ['population_est', 'county_code', 'year']\n",
    "\n",
    "#second, get pop change in 2010\n",
    "pop_grow_2010 = pop_grow_00_10\n",
    "pop_grow_2010['year'] = 2011\n",
    "pop_grow_2010['STATE'] = pop_grow_2010['STATE'].apply(lambda x: state_fix(x))\n",
    "pop_grow_2010['COUNTY'] = pop_grow_2010['COUNTY'].apply(lambda x: county_fix(x))\n",
    "pop_grow_2010['string_county_code'] = pop_grow_2010['STATE'] + pop_grow_2010['COUNTY']\n",
    "pop_grow_2010['county_code'] = pop_grow_2010['string_county_code'].apply(lambda x: int(x))\n",
    "pop_grow_2010 = pop_grow_2010[['year', 'county_code', 'NPOPCHG_2010','RNATURALINC2010','RINTERNATIONALMIG2010','RDOMESTICMIG2010']]\n",
    "pop_grow_2010.columns = ['year', 'county_code', 'net_pop_change_raw', 'natural_pop_growth_rate', 'intl_migrate_rate', 'dom_migrate_rate']\n",
    "\n",
    "#and merge\n",
    "pop_grow_11 = pd.merge(pop_grow_11, pop_grow_2010, how='left', left_on=['year', 'county_code'], right_on=['year', 'county_code'])\n",
    "\n",
    "#apply function to each year other than 2011 on two .csvs\n",
    "growth_03 = growth_convert(pop_grow_00_10, 2003)\n",
    "growth_04 = growth_convert(pop_grow_00_10, 2004)\n",
    "growth_05 = growth_convert(pop_grow_00_10, 2005)\n",
    "growth_06 = growth_convert(pop_grow_00_10, 2006)\n",
    "growth_07 = growth_convert(pop_grow_00_10, 2007)\n",
    "growth_08 = growth_convert(pop_grow_00_10, 2008)\n",
    "growth_09 = growth_convert(pop_grow_00_10, 2009)\n",
    "growth_10 = growth_convert(pop_grow_00_10, 2010)\n",
    "# growth_11 = growth_convert(pop_grow_00_14, 2011) ## note this requires merging the two csvs because spans the difference\n",
    "growth_12 = growth_convert(pop_grow_10_14, 2012)\n",
    "growth_13 = growth_convert(pop_grow_10_14, 2013)\n",
    "growth_14 = growth_convert(pop_grow_10_14, 2014)\n",
    "#merge all population growth features - including special 2011\n",
    "grow_03_14 = pd.concat([growth_03, growth_04, growth_05, growth_06, growth_07, growth_08, growth_09, growth_10, pop_grow_11, growth_12, growth_13, growth_14], axis=0)\n",
    "\n",
    "\n",
    "# ## Re-merging deaths to make sure no errors above\n",
    "\n",
    "deaths_simple = pd.read_csv('/Users/HudsonCavanagh/Dropbox/Capstone/cdc_data/deaths_simple.csv', delimiter='\\t', encoding='utf-8')\n",
    "#put last years alcohol\n",
    "deaths_simple = deaths_simple.applymap(lambda x: str(x).replace(\",\", \"\"))\n",
    "deaths_simple = deaths_simple[['County Code', 'Year', 'Deaths', 'Population', 'Crude Rate', 'Age Adjusted Rate']]\n",
    "deaths_simple.columns = ['county_code', 'year', 'deaths_raw', 'cdc_population', 'death_rate_100k_cdc','death_rate_age_adjusted_cdc']\n",
    "\n",
    "deaths_simple['year'] = deaths_simple['year'].apply(lambda x: yearify(x))\n",
    "deaths_simple['county_code'] = deaths_simple['county_code'].apply(lambda x: yearify(x))\n",
    "deaths_simple['deaths_raw'] = deaths_simple['deaths_raw'].apply(lambda x: death_clean(x))\n",
    "deaths_simple['cdc_population'] = deaths_simple['cdc_population'].apply(lambda x: death_clean(x))\n",
    "deaths_simple['death_rate_100k_cdc'] = deaths_simple['death_rate_100k_cdc'].apply(lambda x: death_clean(x))\n",
    "\n",
    "deaths_pop = deaths_simple.groupby(by=['year', 'county_code']).sum()\n",
    "deaths_pop.reset_index(inplace=1)\n",
    "\n",
    "#merging population\n",
    "deaths_pop = pd.merge(deaths_pop, pop_03_14, how='left', left_on=['year', 'county_code'], right_on=['year', 'county_code'])\n",
    "# deaths_pop['population'] = deaths_pop['population_x'] #removed b/c order changed, no longer necessary\n",
    "\n",
    "#merging poverty\n",
    "\n",
    "deaths_pop = pd.merge(deaths_pop, pov_county_year_03_14, how='left', left_on=['year','county_code'], right_on=['year','county_code'])\n",
    "\n",
    "#merging unemployment\n",
    "\n",
    "deaths_pop = pd.merge(deaths_pop, unemp_03_14, how='left', left_on=['year', 'county_code'], right_on=['year', 'county_code'])\n",
    "\n",
    "# merging fentanyl\n",
    "\n",
    "deaths_pop = pd.merge(deaths_pop, fent_13_14, how='left', left_on=['year'], right_on=['year'])\n",
    "\n",
    "#merging population growth\n",
    "\n",
    "deaths_pop = pd.merge(deaths_pop, grow_03_14, how='left', left_on=['year', 'county_code'], right_on=['year', 'county_code'])\n",
    "deaths_pop.dropna(axis=0, subset=['working_pop', 'employed_pop', 'deaths_raw', 'population_est'], inplace=1)\n",
    "\n",
    "#note - need to extract state (str) to merge narcan/ naxo features (below)\n",
    "\n",
    "###calculations that required merge\n",
    "deaths_pop['drug_death_rate_100k_cdc']  = deaths_pop['deaths_raw']/(deaths_pop['cdc_population']/100000)\n",
    "deaths_pop['drug_death_rate_100k']  = deaths_pop['deaths_raw']/(deaths_pop['population_est']/100000) #accepting this as target\n",
    "deaths_pop['perc_pop_employed'] = deaths_pop['employed_pop']/deaths_pop['population_est']\n",
    "deaths_pop['unemp_rate'] = 1 - (deaths_pop['employed_pop']/deaths_pop['working_pop']) #this is good\n",
    "deaths_pop['perc_pop_working'] = deaths_pop['working_pop']/deaths_pop['population_est']\n",
    "deaths_pop['pov_rate'] = deaths_pop['pov_count_tot']/ deaths_pop['population_est']\n",
    "deaths_pop['pov_rate_youth'] = deaths_pop['pov_youth_count_0-17']/ deaths_pop['population_est']\n",
    "deaths_pop['pop_change_rate'] = deaths_pop['net_pop_change_raw'] / deaths_pop['population_est']\n",
    "deaths_pop['drug_death_rate_100k_diff_cdc_pop'] = deaths_pop['drug_death_rate_100k_cdc'] - deaths_pop['drug_death_rate_100k']\n",
    "deaths_pop['constant'] = 1.0\n",
    "\n",
    "deaths_pop.drop(1391, inplace=1)#noticed this is an outliar in plot below Coconino, AZ  pop listed as 1196.0\n",
    "deaths_pop.dropna(axis=0, subset=['working_pop', 'employed_pop'], inplace=1)\n",
    "\n",
    "##### Readding county names & states, adding state and year as dummy variables\n",
    "\n",
    "deaths_pop[deaths_pop['county_code']==1055]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-a0f076e259f0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a0f076e259f0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    stop stop\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "deaths_pop['county_code'] = deaths_pop['county_code'].apply(lambda x: int(x))\n",
    "deaths_pop['county_name'] = deaths_pop['county_code'].apply(lambda x: name_codes[x])\n",
    "deaths_pop['state'] = deaths_pop['county_name'].apply(lambda x: state_extract(x))\n",
    "\n",
    "\n",
    "state_dums = pd.get_dummies(deaths_pop['state'])\n",
    "deaths_pop = pd.concat([deaths_pop,state_dums], axis = 1)\n",
    "\n",
    "\n",
    "deaths_pop = floater(deaths_pop) #convert any remaining fields to float\n",
    "deaths_pop['year'].apply(lambda x: str(x)) #need year to be in string for dummies\n",
    "year_dums = pd.get_dummies(deaths_pop['year'])\n",
    "deaths_pop = pd.concat([deaths_pop,year_dums], axis = 1) #not using these anymore in features\n",
    "\n",
    "#merging naxolone access policies (Narcan)\n",
    "\n",
    "deaths_pop = pd.merge(deaths_pop, naxo, how='left', left_on=['year', 'state'], right_on=['year', 'state'])\n",
    "\n",
    "\n",
    "## Adding auto-regressive features\n",
    "\n",
    "deaths_pop = deaths_pop.sort_values(by=['county_code', 'year'])\n",
    "deaths_pop['prior_year_death_rate_100k'] = deaths_pop['drug_death_rate_100k'].shift(1)\n",
    "deaths_pop['pre_prior_year_death_rate_100k'] = deaths_pop['drug_death_rate_100k'].shift(2)\n",
    "deaths_pop['prior_year_death_growth_rate'] = (deaths_pop['prior_year_death_rate_100k'] - deaths_pop['pre_prior_year_death_rate_100k'])/deaths_pop['pre_prior_year_death_rate_100k']\n",
    "deaths_pop['three_back_year_death_rate_100k'] = deaths_pop['drug_death_rate_100k'].shift(3)\n",
    "deaths_pop['two_year_period_death_growth_rate'] = (deaths_pop['prior_year_death_rate_100k'] - deaths_pop['three_back_year_death_rate_100k'])/ deaths_pop['three_back_year_death_rate_100k']\n",
    "deaths_pop['two_three_year_death_growth_rate'] = (deaths_pop['pre_prior_year_death_rate_100k']-deaths_pop['three_back_year_death_rate_100k'])/ deaths_pop['three_back_year_death_rate_100k']\n",
    "deaths_pop['years_since_14'] = 2014 - deaths_pop['year']\n",
    "\n",
    "\n",
    "\n",
    "deaths_pop.drop(886, inplace=1)\n",
    "deaths_pop = deaths_pop.sort_index() #to undo sorting\n",
    "death_df = deaths_pop  ## going to clean death_df to include only counties w/ auto-regressive features\n",
    "\n",
    "\n",
    "death_df['count_county_code'] = np.nan\n",
    "death_df.reset_index(inplace=1)\n",
    "death_df = dict_applier(death_df)\n",
    "\n",
    "deaths_pop = death_df[death_df['count_county_code']>=4] # 4 b/c 3 auto-regressive features\n",
    "\n",
    "\n",
    "##### Implementing Model Below\n",
    "\n",
    "\n",
    "median_drug_rate = np.median(deaths_pop['drug_death_rate_100k'])\n",
    "avg_drug_rate = np.mean(deaths_pop['drug_death_rate_100k'])\n",
    "\n",
    "y = deaths_pop['drug_death_rate_100k']\n",
    "\n",
    "# v1 features = deaths_pop[['year','pop_sub_15_prop','pop_15-34_prop','pop_35-54_prop','pop_55+_prop','pop_asian_prop', 'pop_white_prop','pop_black_prop','pop_hisp_prop','population','med_hh_income', 'unemp_rate', 'perc_pop_working', 'pov_rate', 'pov_rate_youth', 'constant', 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO','CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME','MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'NE', 'NH','NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI','SC','SD', 'TN', 'TX', 'UT', 'VA', 'WA', 'WI', 'WV', 'WY']]\n",
    "# v2 features, removed hisp, years, population below = deaths_pop[['pop_sub_15_prop','pop_15-34_prop','pop_35-54_prop','pop_55+_prop','pop_asian_prop', 'pop_white_prop','pop_black_prop','pop_hisp_prop','population','med_hh_income', 'unemp_rate', 'perc_pop_working', 'pov_rate', 'pov_rate_youth', 'constant', 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO','CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME','MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'NE', 'NH','NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI','SC','SD', 'TN', 'TX', 'UT', 'VA', 'WA', 'WI', 'WV', 'WY', 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]]\n",
    "\n",
    "features = deaths_pop[['prior_year_death_rate_100k', 'pre_prior_year_death_rate_100k', 'prior_year_death_growth_rate','three_back_year_death_rate_100k', 'two_year_period_death_growth_rate', 'two_three_year_death_growth_rate', 'national_fentanyl_seizures', 'naxo_crim', 'naxo_civ', 'naxo_third', 'pop_sub_15_prop','pop_15-34_prop','pop_35-54_prop','pop_55+_prop','pop_asian_prop', 'pop_white_prop','pop_black_prop','med_hh_income', 'unemp_rate', 'perc_pop_working', 'pov_rate', 'pov_rate_youth', 'population_est', 'pop_change_rate','natural_pop_growth_rate', 'intl_migrate_rate', 'dom_migrate_rate', 'constant', 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO','CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME','MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'NE', 'NH','NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI','SC','SD', 'TN', 'TX', 'UT', 'VA', 'WA', 'WI', 'WV', 'WY']]\n",
    "#8/25 added autoregressive features\n",
    "\n",
    "features.to_csv('/Users/HudsonCavanagh/Dropbox/Capstone/csv_output/full_features_cdc_states_v7.csv')\n",
    "\n",
    "X = StandardScaler().fit_transform(features)\n",
    "X = pd.DataFrame(X, columns=features.columns)\n",
    "\n",
    "#will be using cross_val, this is to validate most successful model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=50)\n",
    "\n",
    "\n",
    "#PCA for later plots & model testing\n",
    "\n",
    "pca_2 = PCA(n_components=2) # for visualization\n",
    "pca_3 = PCA(n_components=3)\n",
    "pca_4 = PCA(n_components=4)\n",
    "xPC_2 = pca_2.fit_transform(X) # for visualization\n",
    "xPC_3 = pca_3.fit_transform(X)\n",
    "xPC_4 = pca_4.fit_transform(X)\n",
    "\n",
    "stratk = StratifiedKFold(y, n_folds=10, shuffle=True, random_state=66) #for cross_val\n",
    "\n",
    "dt_simple = DecisionTreeRegressor(min_samples_leaf=4, min_samples_split=6, max_depth=6)\n",
    "dt_simple.fit(X,y)\n",
    "s_dt = cross_val_score(dt_simple, X, y, cv=stratk, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "print(\"Decision Tree CV score\", s_dt.mean())\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_cv = RandomForestRegressor()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "ypred_rf = rf.predict(X_test)\n",
    "s_rf_cv = cross_val_score(rf_cv, X, y, cv=stratk, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "print(\"Random Forest CV score\", s_rf_cv.mean())\n",
    "\n",
    "# code for GS RF model\n",
    "\n",
    "# gs_rf = GridSearchCV(rf, {'min_samples_split': split_vals, 'max_depth': depths, 'max_features': max_feats}, cv=15)\n",
    "# s_gs_rf_cv = cross_val_score(gs_rf, X, y, cv=stratk, n_jobs=-1)\n",
    "# s_gs_rf_cv.mean()\n",
    "\n",
    "\n",
    "#Bagging/ Bootstrapped Model\n",
    "bc_cv = BaggingRegressor(n_estimators=100)\n",
    "# bc_cv.fit(X_train,y_train)\n",
    "s_bc = cross_val_score(bc_cv, X, y, cv=stratk, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Bagging Trees\", s_bc.mean(), s_bc.std()))\n",
    "\n",
    "# ## Looking at Feature Importance\n",
    "rel_feature_import = sorted(zip(rf.feature_importances_, features.columns), reverse=True)\n",
    "rel_feature_import = pd.DataFrame(rel_feature_import)\n",
    "print(\"RF feature importance:\", rel_feature_import) #pop was .74 before outliar removal\n",
    "\n",
    "\n",
    "# ## Implementing Kaggle-Darling XG Boost\n",
    "\n",
    "xg_model = xgboost.XGBRegressor()\n",
    "xg_model_cv = xgboost.XGBRegressor()\n",
    "xg_model.fit(X_train, y_train)\n",
    "s_xg_cv = cross_val_score(xg_model_cv, X, y, cv=stratk, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred_xg = xg_model.predict(X_test)\n",
    "print(\"XG Boost cross_val\", s_xg_cv.mean())\n",
    "\n",
    "\n",
    "# ### Implementing GradientBoosting Regressor to see if this is diff from XG Boost\n",
    "\n",
    "gb_tree = GradientBoostingRegressor(n_estimators=1000)\n",
    "gb_tree_cv = GradientBoostingRegressor(n_estimators=1000, max_depth=10)\n",
    "\n",
    "gb_tree.fit(X_train, y_train)\n",
    "ypred_gbtree = gb_tree.predict(X_test)\n",
    "\n",
    "\n",
    "s_gb_tree_cv = cross_val_score(gb_tree_cv, X, y, cv=stratk, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "print(\"Gradient Boosted cross_val\", s_gb_tree_cv.mean())\n",
    "\n",
    "\n",
    "###### Looking at the biggest difference between predicted and observed values - commented out\n",
    "# ypred_gbtree = pd.DataFrame(ypred_gbtree)\n",
    "# y_test = pd.DataFrame(y_test)\n",
    "# pred_act_diff = y_test.merge(deaths_pop, how='left', left_index=1, right_index=1)\n",
    "# pred_act_diff['death_rate_actual_ytest'] = pred_act_diff['drug_death_rate_100k_x']\n",
    "# pred_act_diff = pred_act_diff.merge(ypred_gbtree, how='left', left_index=1, right_index=1)\n",
    "# pred_act_diff['death_rate_pred_gb'] = pred_act_diff[0]\n",
    "# pred_act_diff = pred_act_diff[pred_act_diff['death_rate_pred_gb'].isnull()==False]\n",
    "# pred_act_diff['residual'] = pred_act_diff['death_rate_pred_gb'] - pred_act_diff['death_rate_actual_ytest']\n",
    "#\n",
    "# #note positive means that actual > predicted\n",
    "# pred_act_diff[['residual', 'death_rate_pred_gb', 'death_rate_actual_ytest', 'county_name', 'year']]\n",
    "#\n",
    "# predict_eval = pred_act_diff[['residual', 'death_rate_pred_gb', 'death_rate_actual_ytest', 'county_name', 'year']].sort('residual')\n",
    "# # predict_eval.head(5)\n",
    "# # predict_eval.tail(5)\n",
    "\n",
    "\n",
    "\n",
    "## Now going to use Classification models - Random Forest and XG Boost\n",
    "\n",
    "y_bin = y.apply(lambda x: 1 if x > median_drug_rate else 0) # new target for classification\n",
    "rf_c_cv = RandomForestClassifier()\n",
    "s_rf_cv = cross_val_score(rf_c_cv, X, y_bin, cv=stratk, n_jobs=-1)\n",
    "print(\"random forest, median_drug_rate classifier AUC\", s_rf_cv.mean())\n",
    "\n",
    "xg_class_model_cv = xgboost.XGBClassifier()\n",
    "s_xg_cv = cross_val_score(xg_class_model_cv, X, y_bin, cv=stratk, n_jobs=-1)\n",
    "print(\"XG Boost, median drug rate classifier AUC\", s_xg_cv.mean())\n",
    "\n",
    "\n",
    "## Predicting change in 2014, using 2003-2013 data (hardest model to get accurate)\n",
    "\n",
    "input_2013 = deaths_pop[deaths_pop['year']<2014]\n",
    "input_2013.reset_index(inplace=1) # added this 8/23 to alleviate nans below\n",
    "test_2014 = deaths_pop[deaths_pop['year']==2014]\n",
    "test_2014.reset_index(inplace=1) # added this 8/23 to alleviate nans below\n",
    "\n",
    "#creating regression training\n",
    "y_03_13 = input_2013['drug_death_rate_100k']\n",
    "\n",
    "#regression target\n",
    "y_14 = test_2014['drug_death_rate_100k']\n",
    "\n",
    "#creating classification (bigger/ smaller) train & target\n",
    "y_delta_bin_03_13 = (input_2013['drug_death_rate_100k']-input_2013['prior_year_death_rate_100k']).apply(lambda x: 1 if x >= 0 else 0) #one means increase\n",
    "y_delta_bin_14 = (test_2014['drug_death_rate_100k']-test_2014['prior_year_death_rate_100k']).apply(lambda x: 1 if x >= 0 else 0) #one means increase\n",
    "\n",
    "#creating regression train & target for drug rate growth\n",
    "y_delta_grow_03_13 = ((input_2013['drug_death_rate_100k']-input_2013['prior_year_death_rate_100k'])/input_2013['prior_year_death_rate_100k'])\n",
    "y_delta_grow_14 = ((test_2014['drug_death_rate_100k']-test_2014['prior_year_death_rate_100k'])/test_2014['prior_year_death_rate_100k'])\n",
    "\n",
    "\n",
    "#8/22 added prior year death rate as feature\n",
    "features_03_13 = input_2013[['prior_year_death_rate_100k','pre_prior_year_death_rate_100k', 'prior_year_death_growth_rate','three_back_year_death_rate_100k', 'two_year_period_death_growth_rate', 'two_three_year_death_growth_rate', 'national_fentanyl_seizures','naxo_crim', 'naxo_civ', 'naxo_third', 'pop_sub_15_prop','pop_15-34_prop','pop_35-54_prop','pop_55+_prop','pop_asian_prop', 'pop_white_prop','pop_black_prop','med_hh_income', 'unemp_rate', 'perc_pop_working', 'pov_rate', 'pov_rate_youth', 'population_est', 'pop_change_rate','natural_pop_growth_rate', 'intl_migrate_rate', 'dom_migrate_rate', 'constant', 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO','CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME','MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'NE', 'NH','NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI','SC','SD', 'TN', 'TX', 'UT', 'VA', 'WA', 'WI', 'WV', 'WY']]\n",
    "features_14 = test_2014[['prior_year_death_rate_100k','pre_prior_year_death_rate_100k', 'prior_year_death_growth_rate','three_back_year_death_rate_100k', 'two_year_period_death_growth_rate', 'two_three_year_death_growth_rate','national_fentanyl_seizures','naxo_crim', 'naxo_civ', 'naxo_third', 'pop_sub_15_prop','pop_15-34_prop','pop_35-54_prop','pop_55+_prop','pop_asian_prop', 'pop_white_prop','pop_black_prop','med_hh_income', 'unemp_rate', 'perc_pop_working', 'pov_rate', 'pov_rate_youth', 'population_est', 'pop_change_rate','natural_pop_growth_rate', 'intl_migrate_rate', 'dom_migrate_rate', 'constant', 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO','CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME','MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'NE', 'NH','NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI','SC','SD', 'TN', 'TX', 'UT', 'VA', 'WA', 'WI', 'WV', 'WY']]\n",
    "X_03_13 = StandardScaler().fit_transform(features_03_13)\n",
    "X_03_13 = pd.DataFrame(X_03_13, columns=features_03_13.columns)\n",
    "\n",
    "X_14 = StandardScaler().fit_transform(features_14)\n",
    "X_14 = pd.DataFrame(X_14, columns=features_14.columns)\n",
    "\n",
    "gb_14 = GradientBoostingRegressor(n_estimators=1000, max_depth=10)\n",
    "gb_14.fit(X_03_13, y_03_13)\n",
    "ypred_14 = gb_14.predict(X_14)\n",
    "\n",
    "print(\"GBoosted rsq score, overall drug death rate regression\", r2_score(y_14, ypred_14))\n",
    "\n",
    "#.6075 before fent feature\n",
    "\n",
    "\n",
    "#logistic regression to test direction of  features\n",
    "\n",
    "logreg_basic = LogisticRegression(solver='liblinear')\n",
    "logreg_basic.fit(X_03_13, y_delta_bin_03_13)\n",
    "y_bin_preds_log = logreg_basic.predict(X_14)\n",
    "\n",
    "\n",
    "\n",
    "# X_03_13_ = pd.merge(X_03_13_mod, state_features_train, how='left', left_index=1, right_index=1) -- pretty sure to be removed\n",
    "\n",
    "\n",
    "# Grid Search implementation on LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "C_vals = [0.01, 0.1, .25, 0.5, 0.75, 1.0, 2.5, 5.0, 10.0]\n",
    "penalties = ['l1','l2']\n",
    "\n",
    "gs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, verbose=False, cv=15)\n",
    "gs.fit(X_03_13, y_delta_bin_03_13)\n",
    "y_bin_preds_gs_logis = gs.predict(X_14)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "lr_basic_cm = confusion_matrix(y_delta_bin_14, y_bin_preds_gs_logis)\n",
    "lr_basic_cm_df = pd.DataFrame(lr_basic_cm)\n",
    "print(lr_basic_cm_df)\n",
    "\n",
    "coefs_variables = pd.DataFrame(zip(gs.coef_[0], X_03_13.columns))\n",
    "coefs_variables.columns = ['coef', 'feature']\n",
    "coefs_variables['abs_coef'] = abs(coefs_variables['coef'])\n",
    "coefs_variables = coefs_variables.sort_values(by='abs_coef', ascending=0)\n",
    "print(\"coefficients of logistic regressions:\", coefs_variables)\n",
    "\n",
    "\n",
    "# ## How accurately can we classify communities as compared to the median rate?\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "y_bin_03_13 = y_03_13.apply(lambda x: 1 if x > median_drug_rate else 0)\n",
    "y_bin_14_obs = y_14.apply(lambda x: 1 if x > median_drug_rate else 0)\n",
    "\n",
    "\n",
    "xg_class_model_14 = xgboost.XGBClassifier()\n",
    "xg_class_model_14.fit(X_03_13, y_bin_03_13)\n",
    "y_class_14 = xg_class_model_14.predict(X_14)\n",
    "\n",
    "xg_14_cm = confusion_matrix(y_bin_14_obs, y_class_14, labels=xg_class_model_14.classes_)\n",
    "# xg_14_cm = pd.DataFrame(xg_14_cm, columns=xg_class_model_14.classes_, index=xg_class_model_14.classes_)\n",
    "feat_import_XG_class_14 = pd.Series(zip(X_03_13, xg_class_model_14.feature_importances_))\n",
    "print(feat_import_XG_class_14)\n",
    "\n",
    "#Using tree-based feature importance to reduce my features -- predicting against median\n",
    "\n",
    "xg_class_model_14_select = SelectFromModel(xg_class_model_14, prefit=True)\n",
    "X_03_13_new = xg_class_model_14_select.transform(X_03_13)\n",
    "X_14_new = xg_class_model_14_select.transform(X_14)\n",
    "\n",
    "\n",
    "xg_class_model_14_mod = xgboost.XGBClassifier()\n",
    "xg_class_model_14_mod.fit(X_03_13_new, y_bin_03_13)\n",
    "y_class_xg_14_mod = xg_class_model_14_mod.predict(X_14_new)\n",
    "xg_14_mod_cm = confusion_matrix(y_class_xg_14_mod, y_bin_14_obs, labels=xg_class_model_14.classes_)\n",
    "\n",
    "\n",
    "print('f1:', sklearn.metrics.f1_score(y_bin_14_obs,y_class_xg_14_mod),\n",
    "'accuracy:', accuracy_score(y_bin_14_obs,y_class_xg_14_mod))\n",
    "\n",
    "\n",
    "# plot_confusion_matrix(xg_14_mod_cm)\n",
    "\n",
    "\n",
    "print('f1:', sklearn.metrics.f1_score(y_bin_14_obs,y_class_14),\n",
    "'accuracy:', accuracy_score(y_bin_14_obs,y_class_14))\n",
    "\n",
    "\n",
    "# ### How accurately can we predict the directional movement of communities in 2014, e.g. can our model predict which communties were going to get worse in 2014?\n",
    "\n",
    "xg_delta_model_14 = xgboost.XGBClassifier()\n",
    "xg_delta_model_14.fit(X_03_13, y_delta_bin_03_13)\n",
    "y_delta_14_pred = xg_delta_model_14.predict(X_14)\n",
    "\n",
    "xg_delta_14_cm = confusion_matrix(y_delta_bin_14, y_delta_14_pred)\n",
    "# xg_14_cm = pd.DataFrame(xg_14_cm, columns=xg_class_model_14.classes_, index=xg_class_model_14.classes_)\n",
    "plot_confusion_matrix(xg_delta_14_cm)\n",
    "\n",
    "\n",
    "print('f1:', sklearn.metrics.f1_score(y_delta_bin_14, y_delta_14_pred),\n",
    "'accuracy:', accuracy_score(y_delta_bin_14, y_delta_14_pred))\n",
    "\n",
    "\n",
    "\n",
    "xg_delta_model_14_select = SelectFromModel(xg_delta_model_14, prefit=True)\n",
    "X_03_13_new_delta = xg_delta_model_14_select.transform(X_03_13)\n",
    "X_14_new_delta = xg_delta_model_14_select.transform(X_14)\n",
    "\n",
    "\n",
    "xg_delta_model_14_mod = xgboost.XGBClassifier()\n",
    "xg_delta_model_14_mod.fit(X_03_13_new_delta, y_delta_bin_03_13)\n",
    "y_delta_xg_14_mod = xg_delta_model_14_mod.predict(X_14_new_delta)\n",
    "xg_14_mod_cm = confusion_matrix(y_delta_bin_14, y_class_xg_14_mod, labels=xg_class_model_14.classes_)\n",
    "\n",
    "print(\"XG Boost classification model auc score ('14 delta'):\", sklearn.metrics.roc_auc_score(y_delta_bin_14,y_class_xg_14_mod),\n",
    "      \"naive auc score:\", sklearn.metrics.roc_auc_score(y_delta_bin_14, naive_guesses))\n",
    "\n",
    "# xg_bin_delta_14_cm = confusion_matrix(y_delta_bin_14, naive_guesses)\n",
    "# plot_confusion_matrix(xg_bin_delta_14_cm)\n",
    "\n",
    "\n",
    "gb_delta_14 = GradientBoostingRegressor(n_estimators=1000)\n",
    "\n",
    "gb_delta_14.fit(X_03_13, y_delta_grow_03_13)\n",
    "gb_delta_14_ypred = gb_delta_14.predict(X_14)\n",
    "r2_score(y_delta_grow_14, gb_delta_14_ypred)\n",
    "\n",
    "print(y_delta_grow_14.mean(), gb_delta_14_ypred.mean())\n",
    "\n",
    "\n",
    "# In[108]:\n",
    "\n",
    "gb_delta_model_14_select = SelectFromModel(gb_delta_14, prefit=True)\n",
    "X_03_13_new_delta_gb = gb_delta_model_14_select.transform(X_03_13)\n",
    "X_14_new_delta_gb = gb_delta_model_14_select.transform(X_14)\n",
    "\n",
    "#lets try this with both the features from X_03_13_new_delta_gb and X_03_13_new_delta (same for test)\n",
    "\n",
    "\n",
    "gb_delta_model_14_mod = xgboost.XGBClassifier()\n",
    "gb_delta_model_14_mod.fit(X_03_13_new_delta_gb, y_delta_bin_03_13)\n",
    "y_delta_gb_14_mod = gb_delta_model_14_mod.predict(X_14_new_delta_gb)\n",
    "# gb_14_mod_cm = confusion_matrix(y_delta_grow_14, y_delta_gb_14_mod, labels=xg_class_model_14.classes_)\n",
    "\n",
    "r2_score(y_delta_grow_14, y_delta_gb_14_mod)\n",
    "\n",
    "\n",
    "# In[109]:\n",
    "\n",
    "gb_delta_model_14_xg_feats = xgboost.XGBClassifier()\n",
    "gb_delta_model_14_xg_feats.fit(X_03_13_new_delta, y_delta_bin_03_13)\n",
    "y_delta_gb_14_xg_feats = gb_delta_model_14_xg_feats.predict(X_14_new_delta)\n",
    "# gb_14_mod_cm = confusion_matrix(y_delta_grow_14, y_delta_gb_14_mod, labels=xg_class_model_14.classes_)\n",
    "\n",
    "r2_score(y_delta_grow_14, y_delta_gb_14_xg_feats)\n",
    "\n",
    "\n",
    "# In[110]:\n",
    "\n",
    "sklearn.metrics.f1_score(y_delta_gb_14_xg_feats, y_delta_bin_14)\n",
    "\n",
    "\n",
    "print('f1:', sklearn.metrics.f1_score(y_delta_bin_14, y_delta_gb_14_mod),\n",
    "'accuracy:', accuracy_score(y_delta_bin_14, y_delta_gb_14_mod))\n",
    "\n",
    "\n",
    "### Constructing Final Batch of Features\n",
    "\n",
    "features_03_13_mod = input_2013[['prior_year_death_rate_100k','pre_prior_year_death_rate_100k', 'prior_year_death_growth_rate','three_back_year_death_rate_100k', 'two_year_period_death_growth_rate', 'two_three_year_death_growth_rate','pop_sub_15_prop','pop_15-34_prop','pop_35-54_prop','pop_55+_prop','pop_asian_prop', 'pop_white_prop','pop_black_prop','med_hh_income', 'unemp_rate', 'perc_pop_working', 'pov_rate', 'pov_rate_youth', 'population_est', 'pop_change_rate','natural_pop_growth_rate', 'intl_migrate_rate', 'dom_migrate_rate', 'constant']]\n",
    "\n",
    "state_features_train = input_2013[['AK', 'AL', 'AR', 'AZ', 'CA', 'CO','CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME','MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'NE', 'NH','NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI','SC','SD', 'TN', 'TX', 'UT', 'VA', 'WA', 'WI', 'WV', 'WY']]\n",
    "state_features_test = test_2014[['AK', 'AL', 'AR', 'AZ', 'CA', 'CO','CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME','MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'NE', 'NH','NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI','SC','SD', 'TN', 'TX', 'UT', 'VA', 'WA', 'WI', 'WV', 'WY']]\n",
    "\n",
    "\n",
    "features_14_mod = test_2014[['prior_year_death_rate_100k','pre_prior_year_death_rate_100k', 'prior_year_death_growth_rate','three_back_year_death_rate_100k', 'two_year_period_death_growth_rate', 'two_three_year_death_growth_rate', 'pop_sub_15_prop','pop_15-34_prop','pop_35-54_prop','pop_55+_prop','pop_asian_prop', 'pop_white_prop','pop_black_prop','med_hh_income', 'unemp_rate', 'perc_pop_working', 'pov_rate', 'pov_rate_youth', 'population_est', 'pop_change_rate','natural_pop_growth_rate', 'intl_migrate_rate', 'dom_migrate_rate', 'constant']]\n",
    "\n",
    "X_03_13_mod = StandardScaler().fit_transform(features_03_13_mod)\n",
    "X_03_13_mod = pd.DataFrame(X_03_13_mod, columns=features_03_13_mod.columns)\n",
    "\n",
    "# print(X_03_13_mod.isnull().sum().sum()) -- none\n",
    "\n",
    "X_14_mod = StandardScaler().fit_transform(features_14_mod)\n",
    "X_14_mod = pd.DataFrame(X_14_mod, columns=features_14_mod.columns)\n",
    "\n",
    "X_03_13_mod = pd.merge(X_03_13_mod, state_features_train, how='left', left_index=1, right_index=1)\n",
    "# print(X_03_13_mod.isnull().sum().sum()) -- none\n",
    "X_14_mod = pd.merge(X_14_mod, state_features_test, how='left', left_index=1, right_index=1)\n",
    "\n",
    "gb_delta_14_mod = GradientBoostingRegressor(n_estimators=10000, learning_rate=.02)\n",
    "gb_delta_14_mod.fit(X_03_13_mod, y_delta_grow_03_13) # works\n",
    "gb_delta_14_ypred_mod = gb_delta_14.predict(X_14_mod)\n",
    "\n",
    "print(\"2014 delta regression, GBoost r2 score:\", r2_score(gb_delta_14_ypred_mod, y_delta_grow_14))\n",
    "\n",
    "# gb_delta_14_ypred_mod = pd.DataFrame(gb_delta_14_ypred_mod)\n",
    "# worst_off_2014 = pd.merge(test_2014, gb_delta_14_ypred, how='left', left_index=1, right_index=1)\n",
    "\n",
    "\n",
    "\n",
    "## Grid Search Optimizing Gradient Boosted Trees -- commented out because takes too long\n",
    "#\n",
    "# max_feature_list = ['auto', 'sqrt']\n",
    "# param_test = {'max_depth':range(3,15,2), 'max_features':max_feature_list, 'min_samples_split':range(3,9), 'min_samples_leaf':range(2,8)}\n",
    "#\n",
    "# gb_delta_14_reg = GradientBoostingRegressor(n_estimators=1000, learning_rate=.05)\n",
    "# gb_delta_14_class = GradientBoostingClassifier(n_estimators=1000, learning_rate=.05)\n",
    "#\n",
    "# gs_delta_14_reg = GridSearchCV(gb_delta_14_reg, param_test, verbose=False, cv=15)\n",
    "# gs_delta_14_reg.fit(X_03_13_mod, y_delta_grow_03_13)\n",
    "# y_delta_14_gs_preds = gs_delta_14_reg.predict(X_14_mod)\n",
    "#\n",
    "# print(\"2014 delta regression, GBoost r2 score:\", r2_score(gb_delta_14_ypred_mod, y_delta_grow_14)\n",
    "#\n",
    "# print(gs_delta_14_reg.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 New",
   "language": "python",
   "name": "stats_mods_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
