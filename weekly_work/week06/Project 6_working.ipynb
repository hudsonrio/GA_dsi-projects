{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from imdbpie import Imdb\n",
    "\n",
    "\n",
    "# imdb = Imdb()\n",
    "# imdb = Imdb(anonymize=True) # to proxy requests\n",
    "\n",
    "# Creating an instance with caching enabled\n",
    "# Note that the cached responses expire every 2 hours or so.\n",
    "# The API response itself dictates the expiry time)\n",
    "imdb = Imdb(cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_250_1 = imdb.top_250()\n",
    "top_250_1 = pd.DataFrame(top_250_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>can_rate</th>\n",
       "      <th>image</th>\n",
       "      <th>num_votes</th>\n",
       "      <th>rating</th>\n",
       "      <th>tconst</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>{u'url': u'http://ia.media-imdb.com/images/M/M...</td>\n",
       "      <td>1679054</td>\n",
       "      <td>9.3</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>feature</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>{u'url': u'http://ia.media-imdb.com/images/M/M...</td>\n",
       "      <td>1148812</td>\n",
       "      <td>9.2</td>\n",
       "      <td>tt0068646</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>feature</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>{u'url': u'http://ia.media-imdb.com/images/M/M...</td>\n",
       "      <td>785614</td>\n",
       "      <td>9.0</td>\n",
       "      <td>tt0071562</td>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>feature</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>{u'url': u'http://ia.media-imdb.com/images/M/M...</td>\n",
       "      <td>1664202</td>\n",
       "      <td>9.0</td>\n",
       "      <td>tt0468569</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>feature</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>{u'url': u'http://ia.media-imdb.com/images/M/M...</td>\n",
       "      <td>859341</td>\n",
       "      <td>8.9</td>\n",
       "      <td>tt0108052</td>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>feature</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  can_rate                                              image  num_votes  \\\n",
       "0     True  {u'url': u'http://ia.media-imdb.com/images/M/M...    1679054   \n",
       "1     True  {u'url': u'http://ia.media-imdb.com/images/M/M...    1148812   \n",
       "2     True  {u'url': u'http://ia.media-imdb.com/images/M/M...     785614   \n",
       "3     True  {u'url': u'http://ia.media-imdb.com/images/M/M...    1664202   \n",
       "4     True  {u'url': u'http://ia.media-imdb.com/images/M/M...     859341   \n",
       "\n",
       "   rating     tconst                     title     type  year  \n",
       "0     9.3  tt0111161  The Shawshank Redemption  feature  1994  \n",
       "1     9.2  tt0068646             The Godfather  feature  1972  \n",
       "2     9.0  tt0071562    The Godfather: Part II  feature  1974  \n",
       "3     9.0  tt0468569           The Dark Knight  feature  2008  \n",
       "4     8.9  tt0108052          Schindler's List  feature  1993  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_250_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "top_250 = imdb.top_250()\n",
    "top_250 = pd.DataFrame(top_250)\n",
    "\n",
    "def review_find(imdb_id):\n",
    "    try:\n",
    "        return(imdb.get_title_reviews(imdb_id, max_results=30).strip(\"[<>]\"))\n",
    "    except:\n",
    "        return('error')\n",
    "\n",
    "def id_find(title):\n",
    "    try:\n",
    "        movie = imdb.search_for_title(title)\n",
    "        return (movie[0]['imdb_id'])\n",
    "    except:\n",
    "        return('error')\n",
    "\n",
    "def score_find(imdb_id):\n",
    "    try:\n",
    "        return(imdb.rating)\n",
    "    except:\n",
    "        return(\"error\")\n",
    "    \n",
    "def cert_find(imdb_id):\n",
    "    try:\n",
    "        return(imdb.certification)\n",
    "    except:\n",
    "        return(\"error\")\n",
    "\n",
    "top_250['imdb_id'] = top_250['title'].apply(id_find)\n",
    "top_250['reviews'] = top_250['imdb_id'].apply(review_find)\n",
    "top_250['num_reviews'] = top_250['reviews'].apply(lambda x: len(x.split(\"Review:\"))) #havent checked this\n",
    "\n",
    "top_250['num_reviews_scaled'] = preprocessing.StandardScaler().fit_transform(top_250['num_reviews'])\n",
    "\n",
    "top_250['rating_scaled'] = preprocessing.StandardScaler().fit_transform(top_250['rating'])\n",
    "\n",
    "\n",
    "top_250.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def certification_find(imdb_id):\n",
    "    try:\n",
    "        instance = imdb.get_title_by_id(imdb_id)\n",
    "        return(instance.certification)\n",
    "    except:\n",
    "        return(\"error\")\n",
    "\n",
    "top_250['certification'] = top_250['imdb_id'].apply(certification_find)\n",
    "\n",
    "cert_dums = pd.get_dummies(top_250['certification']\n",
    "top_250 = pd.concat([top_250, cert_dums], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##couldn't get reviews this way\n",
    "# def review_find1(imdb_id):\n",
    "#     try:\n",
    "#         return(imdb.get_title_reviews(imdb_id, max_results=10))\n",
    "#     except:\n",
    "#         return('error')\n",
    "    \n",
    "# top_250['review_long'] = top_250['imdb_id'].apply(review_find1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#going to use omdb for reviews\n",
    "# def plot_find(imdb_id):\n",
    "#     try:\n",
    "#         instance = imdb.get_title_by_id(imdb_id)\n",
    "#         return(imdb.get_plot(instance, max_results=10))\n",
    "#     except:\n",
    "#         return('error')\n",
    "    \n",
    "# top_250['plot'] = top_250['imdb_id'].apply(plot_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reviews don't work here, going to use omdb api\n",
    "# for i in top_250['reviews']:\n",
    "#     for i2 in i:\n",
    "#         print(i2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def credit_finder(imdb_id):\n",
    "    try:\n",
    "        title = imdb.get_title_by_id(imdb_id)\n",
    "        return(title.credits)\n",
    "    except:\n",
    "        return(\"error\")\n",
    "\n",
    "top_250['credits'] = top_250['imdb_id'].apply(credit_finder)\n",
    "top_250['num_listed_collabs'] = top_250['credits'].apply(lambda x: len(x.split(\"Person:\"))                                                         \n",
    "top_250['scaled_collab_count'] = preprocessing.StandardScaler().fit_transform(top_250['num_listed_collabs'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_counts = top_250.count(axis=1)\n",
    "null_counts_rows = top_250.count(axis=0)\n",
    "null_counts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_counts_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #things to do:\n",
    "\n",
    "\n",
    "# string parse plot\n",
    "# count 'persons' in credits (DONE above)\n",
    "# scale runtime (DONE here)\n",
    "# convert year to date time\n",
    "# convert released to datetime\n",
    "# country to dummies\n",
    "# scale num votes\n",
    "# create dummys for type\n",
    "# parse awards (somehow)\n",
    "#create 'the' variable (DONE BELOW)\n",
    "#need to make sure all data types are right\n",
    "\n",
    "def intminutes(x):\n",
    "    y = x.replace('min', '').strip()\n",
    "    return int(y)\n",
    "\n",
    "def integer_votes(x):\n",
    "    y = x.replace(',', '').strip()\n",
    "    return int(y)\n",
    "\n",
    "\n",
    "\n",
    "top_250['num_votes'] = top_250['num_votes'].apply(integer_votes)\n",
    "top_250['year'] = top_250['Year'].astype(int) #should i make this feature, years since release? 2016-x?\n",
    "top_250['Runtime'] = top_250['Runtime'].apply(intminutes)\n",
    "top_250['runtime_scaled'] = preprocessing.StandardScaler().fit_transform(df1['Runtime'])\n",
    "top_250['num_votes_scaled'] = preprocessing.StandardScaler().fit_transform(top_250['num_votes'])\n",
    "top_250['year_date'] = pd.to_datetime(top_250['Year'])# optional arguement for trying various options out should be on\n",
    "top_250['release_date'] = pd.to_datetime(top_250['Release'])\n",
    "top_250['the'] = top_250['title'].apply(lambda x: 1 if \"the\" in x else 0)\n",
    "top_250['subtitle'] = top_250['title'].apply(lambda x: 1 if \":\" in x else 0)\n",
    "\n",
    "type_dums = pd.get_dummies(top_250['Type'])\n",
    "top_250 = pd.concat([top_250, type_dums], axis = 1)\n",
    "country_dums = pd.get_dummies(top_250['Country'])\n",
    "top_250 = pd.concat([top_250, country_dums], axis = 1)\n",
    "director_dums = pd.get_dummies(top_250['Director'])\n",
    "top_250 = pd.concat([top_250, director_dums], axis = 1)\n",
    "top_250['num_actors'] = top_250.apply(lambda x: len(top_250['Actors'].split(','))\n",
    "genre_dums = pd.get_dummies(top_250['Genre'].split(\", \"))\n",
    "top_250 = pd.concat([top_250, genre_dums], axis =1)\n",
    "top_250['nominated'] = top_250.apply(lambda x: 1 if \"nomin\" in x else 0)\n",
    "top_250['win'] = top_250.apply(lambda x: 1 if any([\"win\", 'won', 'winner', 'awarded', 'received'] in x else 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_counts_rows.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_250.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in top_250['credits']:\n",
    "#     print(i)\n",
    "    \n",
    "#need to split these persons w/ text parsing  \n",
    "#also make feature for # of people involved in the movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_250.columns = ['can_rate','image','num_votes','rating','tconst','Title','type','year','imdb_id','reviews','certification','review_long','credits'] #need to set title column as Title for merging\n",
    "top_250.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function for all at once\n",
    "\n",
    "ids = top_250['imdb_id']\n",
    "#problem here is that its overwritting each row before, the last one comes back successfully\n",
    "#tried again below with for loop outside of function\n",
    "def info_adder(ids, dataframe):\n",
    "    for i in ids:\n",
    "        try:\n",
    "            api_base_url = 'http://www.omdbapi.com/?i={}&plot=full&r=json'.format(i)\n",
    "            api_response = requests.get(api_base_url)\n",
    "            response = json.loads(api_response.text)\n",
    "            new_info = pd.DataFrame(response.items())\n",
    "            new_info.set_index(0, inplace=1)\n",
    "            new_info = new_info.transpose()\n",
    "            new_info.rename(columns={'imdbID':'imdb_id'}, inplace=True)\n",
    "            dataframe = pd.merge(dataframe, new_info, how='left', on='imdb_id', inplace=1)\n",
    "        except:\n",
    "            continue\n",
    "    return dataframe\n",
    "            \n",
    "top_250 = info_adder(ids, top_250)\n",
    "top_250.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#take 2 -- I think this should work\n",
    "\n",
    "def info_adder2(imdb_id, dataframe):\n",
    "    api_base_url = 'http://www.omdbapi.com/?i={}&plot=full&r=json'.format(imbd_id)\n",
    "    api_response = requests.get(api_base_url)\n",
    "    if api_response.status_code != 200:\n",
    "        print(entry, api_response.status_code)\n",
    "    try:\n",
    "        response = json.loads(api_response.text)\n",
    "        new_info = pd.DataFrame(response.items())\n",
    "        new_info.set_index(0, inplace=1)\n",
    "        new_info = new_info.transpose()\n",
    "        new_info.rename(columns={'imdbID':'imdb_id'}, inplace=True)\n",
    "        dataframe = pd.merge(dataframe, new_info, how='left', on='imdb_id', inplace=1)\n",
    "        return(dataframe)\n",
    "    except:\n",
    "        return('error')\n",
    "\n",
    "\n",
    "ids = top_250['imdb_id']\n",
    "#problem here is that its overwritting each row before, the last one comes back successfully\n",
    "\n",
    "\n",
    "for id_example in ids:\n",
    "    top_250 = info_adder2(id_example, top_250)\n",
    "top_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function for all at once\n",
    "# ids = top_250['imdb_id']\n",
    "\n",
    "\n",
    "# # def info_adder(imdb_ids, dataframe):\n",
    "# for i in imdb_ids:\n",
    "# #         try:\n",
    "#     api_base_url = 'http://www.omdbapi.com/?i={}&plot=full&r=json'.format(i)\n",
    "#     print(api_base_url)\n",
    "# #             api_response = requests.get(api_base_url)\n",
    "#             response = json.loads(api_response.text)\n",
    "#             new_info = pd.DataFrame(response.items())\n",
    "#             new_info.set_index(0, inplace=1)\n",
    "#             new_info = new_info.transpose()\n",
    "#             new_info.rename(columns={'imdbID':'imdb_id'}, inplace=True)\n",
    "#             dataframe = pd.merge(dataframe, new_info, how='left', on='imdb_id')\n",
    "#         except:\n",
    "#             continue\n",
    "#     return(dataframe)\n",
    "            \n",
    "# top_250 = info_adder(ids, top_250)\n",
    "# top_250.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###function for one at a time\n",
    "# import string \n",
    "\n",
    "\n",
    "# def info_adder1(imdb_id):\n",
    "#     try:\n",
    "        \n",
    "#         api_base_url = 'http://www.omdbapi.com/?i={}&plot=full&r=json'.format(imdb_id)\n",
    "#         api_response = requests.get(api_base_url)\n",
    "#         response = json.loads(api_response.text)\n",
    "#         new_info = pd.DataFrame(response.items())\n",
    "#         new_info.set_index(0, inplace=1)\n",
    "#         new_info.rename(columns={'imdbID':'imdb_id'}, inplace=True)\n",
    "#         new_info = new_info.transpose()\n",
    "#     except:\n",
    "#         return('error')\n",
    "#     return(new_info)\n",
    "\n",
    "# for i,v in enumerate(top_250['imdb_id']):\n",
    "#      info_adder1(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "The Godfather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_250.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api_base_url = \"http://www.omdbapi.com/?t='Shawshank Redemption'&y=&plot=full&r=json\"\n",
    "api_response = requests.get(api_base_url)\n",
    "response = json.loads(api_response.text)\n",
    "shawshank = pd.DataFrame(response.items())\n",
    "shawshank.set_index(0, inplace=1)\n",
    "shawshank.transpose()\n",
    "# = pd.DataFrame(response)\n",
    "# shawshank.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_gross(entry):\n",
    "    response = requests.get('http://www.imdb.com/title/'+entry)\n",
    "    html = response.text\n",
    "    try:\n",
    "        gross_list = re.findall(\"Gross:</h4>[ ]*\\$([^ ]*)\", html)\n",
    "        gross = int(gross_list[0].replace(',', ''))\n",
    "        print '.',\n",
    "        return gross\n",
    "    except Exception as ex:\n",
    "        print\n",
    "        print ex, entry, response.status_code\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "grosses = [(e, get_gross(e)) for e in ids]\n",
    "df1 = pd.DataFrame(grosses, columns=['imdb_id', 'Gross'])\n",
    "\n",
    "df1 = preprocessing.StandardScaler().fit_transform(df1['Gross'])\n",
    "top_250 = pd.merge(top_250, df1, on=\"imdb_id\", how='left')\n",
    "top_250['gross_scaled'] = preprocessing.StandardScaler().fit_transform(top_250['Gross'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# string parse title and plot\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "cvec_plot = CountVectorizer(stop_words='english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "data_plot = cvec_plot.fit_transform(stemmer.stem(top_250['Plot'])).todense()\n",
    "data_plot.columns()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# len(cvec.get_feature_names())\n",
    "# X_train = pd.DataFrame(cvec.transform(data_train['data']).todense(),\n",
    "#                        columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv_ham = TfidfVectorizer(stop_words='english')\n",
    "tv_ham.fit([ham])\n",
    "\n",
    "tv_ham  = pd.DataFrame(tv_ham.transform([ham]).todense(),\n",
    "             columns=tv_ham.get_feature_names())\n",
    "\n",
    "df_ham_only = tv_ham.transpose().sort_values(0, ascending=False).head(15).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec\n",
    "\n",
    "cv = CountVectorizer(token_pattern=u'(?u)\\\\w+\\.?\\\\w?\\.? \\\\w+')\n",
    "data = cv.fit_transform(df.Actors).todense()\n",
    "columns = ['actor: '+c for c in cv.get_feature_names()]\n",
    "actorsdf = pd.DataFrame(data, columns=columns)\n",
    "actorsdf.head()\n",
    "actorsdf.loc[0,actorsdf.iloc[0] != 0]\n",
    "df = pd.concat([df, actorsdf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# title = imdb.get_title_by_id(\"tt1210166\")\n",
    "# for person in title.credits:\n",
    "#     # check if they are a writer\n",
    "#     if person.token == 'writers':\n",
    "#         print(person.name + ' is a writer')\n",
    "#     else:\n",
    "#         print(person.name + ' is not a writer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing nulls still in df\n",
    "\n",
    "top_250 = top_250.applymap(replace('N/A', np.nan))\n",
    "top_250 = top_250.applymap(replace(None, np.nan))\n",
    "\n",
    "top_250.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Goal Features:\n",
    "\n",
    "# [‘rating_scaled’, ‘year’, ’num_reviews_scaled’, ‘rating_scaled’, ‘G’**??**, ‘PG’,  ‘PG-13’, ‘R’, ‘scaled_collab_count’, ’num_votes_scaled’, ‘release_date'\n",
    "# ‘the’, ‘subtitle’, ‘num_actors’, ‘nominated’, ‘win’, ’gross_scaled’, ]\n",
    "\n",
    "# look at: genre_dums, director_dums, country_dums, type_dums\n",
    "# also look at any other features added with function info_adder2\n",
    "#replace win with a better award feature: make list of awards that are valueable from the data (use text analysis)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_250['constant']=1\n",
    "X = top_250.loc[:,['constant', 'year', 'num_reviews_scaled', 'G', 'PG','PG-13', 'R', 'scaled_collab_count', 'num_votes_scaled', 'release_date','the', 'subtitle', 'num_actors', 'nominated', 'win', 'gross_scaled’]]\n",
    "#make sure to add dummy features\n",
    "y = top_250.loc[:, 'rating'].values\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.isnull().any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "stratk = StratifiedKFold(y, n_folds=10, shuffle=True, random_state=66)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "\n",
    "dt.fit(X_train,y_train)\n",
    "s_dt = cross_val_score(dt, X_scale, y, cv=stratk, n_jobs=-1, scoring='f1')\n",
    "\n",
    "\n",
    "\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s_dt.mean().round(3), s_dt_scale.std().round(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances_dt = pd.DataFrame(dt.feature_importances_,\n",
    "                                   index = X.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)\n",
    "print(feature_importances_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rfc = RandomForestClassifier(class_weight='balanced', n_estimators=1000) #.948 before, .962 after\n",
    "\n",
    "#X_scale = X_train     X_dums = Xd_train\n",
    "rfc.fit(X_train,y_train)\n",
    "s_rf = cross_val_score(rfc, X_scale, y, cv=stratk, n_jobs=-1, scoring='f1')\n",
    "\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Random Forest Scaled\", s_rf_scale.mean().round(3), s_rf_scale.std().round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(n_estimators=1000)\n",
    "\n",
    "bag.fit(X_train,y_train)\n",
    "s_bag = cross_val_score(bag, X_scale, y, cv=stratk, n_jobs=-1, scoring='f1')\n",
    "\n",
    "print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Bagging Tree Scaled\", s_bag.mean().round(3), s_bc_scale.std().round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "def auc_plotting_function(rate1, rate2, rate1_name, rate2_name, curve_name):\n",
    "    AUC = metrics.auc(rate1, rate2)\n",
    "    # Plot of a ROC curve for class 1 (has_cancer)\n",
    "    plt.figure(figsize=[11,9])\n",
    "    plt.plot(rate1, rate2, label=curve_name + ' (area = %0.2f)' % AUC, linewidth=4)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(rate1_name, fontsize=18)\n",
    "    plt.ylabel(rate2_name, fontsize=18)\n",
    "    plt.title(curve_name + ' for salary > ~100,000', fontsize=18)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# plot receiving operator characteristic curve\n",
    "def plot_roc(y_true, y_score):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, y_score)\n",
    "    auc_plotting_function(fpr, tpr, 'False Positive Rate', 'True Positive Rate', 'ROC')\n",
    "\n",
    "\n",
    "plot_roc(y_test, ypred_bc_knn) ## Bagging KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_roc(y_test, ypred_bc_scale) ## Bagging over trees with scaled features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
