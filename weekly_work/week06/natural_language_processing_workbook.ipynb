{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from IPython.core.display import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "PATH = './assets/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Feature Extraction from Text + NLP\n",
    "Week 6| Lesson 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Extract features from free-form text using Scikit Learn\n",
    "- Identify Parts of Speech using NLTK\n",
    "- Remove stop words\n",
    "- Describe how TFIDF works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "\n",
    "* Working with Text\n",
    "* Scikit-Learn Count Vectorizer \n",
    "* Scikit-Learn Hashing Vectorizer\n",
    "* Intro to Natural Language Processing\n",
    "* Advanced NLP with NLTK\n",
    "* Term frequency - Inverse document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So far nearly all our data has looked like this...<br><br>\n",
    "$$ \n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "    feature_1 & feature_2 & feature_3 & feature_4 \\\\\n",
    "    0.1 & 1.5 & 6 & 1.1 \\\\\n",
    "    2.1 & 5.7 & 33 & 9.2 \\\\\n",
    "    0.1 & 2.1 & 41 & 3.1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But what if our data looked like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<small><center>I said a hip hop,<br>\n",
    "The hippie, the hippie,<br>\n",
    "To the hip, hip hop, and you don't stop, a rock it<br>\n",
    "To the bang bang boogie, say, up jump the boogie,<br>\n",
    "To the rhythm of the boogie, the beat.<br>\n",
    "Now, what you hear is not a test - I'm rappin' to the beat,<br>\n",
    "And me, the groove, and my friends are gonna try to move your feet.<br>\n",
    "See, I am Wonder Mike, and I'd like to say hello,<br>\n",
    "To the black, to the white, the red and the brown,<br>\n",
    "The purple and yellow. But first, I gotta<br>\n",
    "Bang bang, the boogie to the boogie,<br>\n",
    "Say up jump the boogie to the bang bang boogie,<br>\n",
    "Let's rock, you don't stop,<br>\n",
    "Rock the rhythm that'll make your body rock.<br>\n",
    "Well so far you've heard my voice but I brought two friends along,<br>\n",
    "And the next on the mic is my man Hank,\n",
    "C'mon, Hank, sing that song!</center></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "Li9hc3NldHMvaW1hZ2VzL2pjaGFuLmpwZw==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(PATH + 'jchan.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## That is the focus of Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question: What are some real-world applications of working with language-based data (text or spoken)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Speech Recognition [Siri]\n",
    "- Machine Translation [Google Translate]\n",
    "- Question Answering [Echo]\n",
    "- Topic Clustering [Google News]\n",
    "- Sentiment Analysis [Social Mention ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<small><center>I said a hip hop,<br>\n",
    "The hippie, the hippie,<br>\n",
    "To the hip, hip hop, and you don't stop, a rock it<br>\n",
    "To the bang bang boogie, say, up jump the boogie,<br>\n",
    "To the rhythm of the boogie, the beat.<br>\n",
    "Now, what you hear is not a test - I'm rappin' to the beat,<br>\n",
    "And me, the groove, and my friends are gonna try to move your feet.<br>\n",
    "See, I am Wonder Mike, and I'd like to say hello,<br>\n",
    "To the black, to the white, the red and the brown,<br>\n",
    "The purple and yellow. But first, I gotta<br>\n",
    "Bang bang, the boogie to the boogie,<br>\n",
    "Say up jump the boogie to the bang bang boogie,<br>\n",
    "Let's rock, you don't stop,<br>\n",
    "Rock the rhythm that'll make your body rock.<br>\n",
    "Well so far you've heard my voice but I brought two friends along,<br>\n",
    "And the next on the mic is my man Hank,\n",
    "C'mon, Hank, sing that song!</center></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "    feature_1 & feature_2 & feature_3 & feature_4 \\\\\n",
    "    0.1 & 1.5 & 6 & 1.1 \\\\\n",
    "    2.1 & 5.7 & 33 & 9.2 \\\\\n",
    "    0.1 & 2.1 & 41 & 3.1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  But, remember sometimes we had something that looked like this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "    feature_1 & feature_2 & feature_3 & feature_4 \\\\\n",
    "    0.1 & 1.5 & 6 & red \\\\\n",
    "    2.1 & 5.7 & 33 & blue \\\\\n",
    "    0.1 & 2.1 & 41 & green \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So what did we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "pd.get_dummies()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>33</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>41</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1   f2  f3     f4\n",
       "0  0.1  1.5   6    red\n",
       "1  2.1  5.7  33   blue\n",
       "2  0.1  2.1  41  green"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'f1': [0.1, 2.1, 0.1], 'f2': [1.5, 5.7, 2.1], 'f3': [6, 33, 41], 'f4': ['red', 'blue', 'green']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1   f2  f3  blue  green  red\n",
       "0  0.1  1.5   6     0      0    1\n",
       "1  2.1  5.7  33     1      0    0\n",
       "2  0.1  2.1  41     0      1    0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_dummies = pd.get_dummies(df['f4']).astype('int')\n",
    "cdf = pd.concat([df.iloc[:,:-1], color_dummies], axis=1)\n",
    "cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## With that coding scheme, we could check for the presence or absence of some feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## But is that it? Is that Enough to Build a Siri or an Echo? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "Li9hc3NldHMvaW1hZ2VzL3Nwb2lsZXIuanBn\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(PATH + 'spoiler.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What might be missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Question: What are some concerns we might have with that approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What we need to answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we get the words into the columns?\n",
    "- How do we deal with the same word multiple times?\n",
    "- What about word order?\n",
    "- What do we do with punctuation?\n",
    "- Should we only use some words?\n",
    "- Do we count every word equally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## All of this and more will be revealed..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Extraction from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A simple example\n",
    "\n",
    "Suppose we are building a spam/ham classifier. \n",
    "Our input is an email corpus, our expected output is a binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521\n",
      "453\n"
     ]
    }
   ],
   "source": [
    "spam = \"\"\"\n",
    "Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "\"\"\"\n",
    "\n",
    "ham = \"\"\"\n",
    "Hello,\\nI am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
    "\"\"\"\n",
    "print(len(spam))\n",
    "print\n",
    "print(len(ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One way to do this might be to add a feature indicating the presence of or absence of a word from our master spammy word list, or to have a feature having the total count of spammy words, or maybe both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Another way to do that might be to get a count for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Something like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 7, 'of': 4, 'and': 3, 'is': 2, 'etc.': 2, 'am': 2, 'an': 2, 'have': 2, 'in': 2, 'your': 2, 'to': 2, 'years': 2, 'with': 2, 'this': 2, 'contact': 2, 'the': 2, 'major': 1, 'old': 1, 'cancer': 1, 'outstanding': 1, 'seven': 1, 'decided': 1, 'through': 1, 'carefully': 1, 'euros(eight': 1, 'seem': 1, 'saw': 1, 'information': 1, 'for': 1, 'euros': 1, 'fifty': 1, '86': 1, 'sum': 1, '\"lukoil\".': 1, 'only': 1, 'pjsc': 1, 'mr.': 1, '2': 1, 'linkedin.': 1, 'will/donate': 1, 'you': 1, 'hundred': 1, 'was': 1, 'personality.': 1, 'chairman': 1, 'profile': 1, 'you.': 1, 'hello,': 1, 'ago.': 1, 'read': 1, 'going': 1, 'thousand': 1, 'million': 1, 'grayfer': 1, 'reason': 1, 'be': 1, 'one': 1, 'why': 1, 'on': 1, 'name': 1, 'week.': 1, '8,750,000.00': 1, 'later': 1, 'board': 1, 'operation': 1, 'will': 1, 'directors': 1, 'diagnosed': 1, 'valery': 1, 'my': 1})\n",
      "\n",
      "\n",
      "Counter({'to': 5, 'you': 4, 'of': 4, 'the': 3, 'and': 2, 'we': 2, 'scientist': 2, 'data': 2, 'i': 2, 'further': 2, 'this': 1, 'regards.': 1, 'find': 1, 'information': 1, 'am': 1, 'an': 1, 'at': 1, 'in': 1, 'our': 1, 'message': 1, 'pleased': 1, 'best': 1, 'if': 1, 'will': 1, 'would': 1, 'with': 1, 'interviews': 1, 'please': 1, 'writing': 1, 'application': 1, 'mr.': 1, 'location': 1, 'passed': 1, 'interview': 1, 'for': 1, 'john': 1, 'date,': 1, 'be': 1, 'hello,': 1, 'x.': 1, 'invite': 1, 'that': 1, 'any': 1, 'interview.': 1, 'regards': 1, 'let': 1, 'know': 1, 'hooli': 1, 'on-site': 1, 'me': 1, 'on': 1, 'your': 1, 'like': 1, 'assistance.': 1, 'attached': 1, 'senior': 1, 'inform': 1, 'smith.': 1, 'can': 1, 'time': 1, 'position': 1, 'first': 1, 'round': 1, 'are': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(spam.lower().split()))\n",
    "print('\\n')\n",
    "print(Counter(ham.lower().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This is called a \"bag-of-words\" approach. We dispense with all the structure and the semantics in the document and treat each as if we had tossed all the words into a bag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can we do this in Scikit-Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit Learn Count Vectorizer\n",
    "\n",
    "Scikit learn offers a Count Vectorizer with many configurable options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char_wb', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer(analyzer='char_wb')\n",
    "cvec.fit([spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>o</th>\n",
       "      <th>e</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>n</th>\n",
       "      <th>r</th>\n",
       "      <th>t</th>\n",
       "      <th>s</th>\n",
       "      <th>l</th>\n",
       "      <th>d</th>\n",
       "      <th>h</th>\n",
       "      <th>y</th>\n",
       "      <th>c</th>\n",
       "      <th>u</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         o   e   a   i   n   r   t   s   l   d   h   y   c   u\n",
       "0  188  39  38  35  35  31  27  26  20  19  18  16  14  13  13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df  = pd.DataFrame(cvec.transform([spam]).todense(),\n",
    "             columns=cvec.get_feature_names())\n",
    "\n",
    "df.transpose().sort_values(0, ascending=False).head(15).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But, wait, what are all those parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```\n",
    "class sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:  <br><br>Spend a couple minutes scanning the documentation to figure out what those parameters do. Take 5 minutes, then share a few takeaways from the documentation in groups. What features stand out to you?\n",
    "\n",
    "[Count Vectorizer Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So, feature size can be an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## One way to fix that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit Learn Hashing Vectorizer\n",
    "\n",
    "### Hashing vectorizer\n",
    "\n",
    "These problems can be solved using the `HashingVectorizer`, which converts a collection of text documents to a matrix of occurrences, calculated with the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing). Each word is mapped to a feature with the use of a [hash function](https://en.wikipedia.org/wiki/Hash_function) that converts it to a hash. If we encounter that word again in the text, it will be converted to the same hash, allowing us to count word occurence without retaining a dictionary in memory. This is very convenient!\n",
    "\n",
    "The main drawback of the this trick is that it's not possible to compute the inverse transform, and thus we lose information on what words the important features correspond to. The hash function employed is the signed 32-bit version of Murmurhash3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:<br><br>Turn spam and ham into a BoW representation using HashingVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HashingVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "         dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "         lowercase=True, n_features=1048576, ngram_range=(1, 1),\n",
       "         non_negative=False, norm=u'l2', preprocessor=None,\n",
       "         stop_words=None, strip_accents=None,\n",
       "         token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hvec = HashingVectorizer()\n",
    "hvec1 = HashingVectorizer()\n",
    "\n",
    "hvec.fit([spam])\n",
    "\n",
    "hvec1.fit([ham])\n",
    "\n",
    "# feature_extraction.text.HashingVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', n_features=1048576, binary=False, norm='l2', non_negative=False, dtype=<class 'numpy.float64'>)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>479532</th>\n",
       "      <th>144749</th>\n",
       "      <th>174171</th>\n",
       "      <th>832412</th>\n",
       "      <th>828689</th>\n",
       "      <th>994433</th>\n",
       "      <th>1005907</th>\n",
       "      <th>170062</th>\n",
       "      <th>675997</th>\n",
       "      <th>959146</th>\n",
       "      <th>784967</th>\n",
       "      <th>637271</th>\n",
       "      <th>51251</th>\n",
       "      <th>134503</th>\n",
       "      <th>947540</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.338062</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.084515</td>\n",
       "      <td>0.084515</td>\n",
       "      <td>0.084515</td>\n",
       "      <td>0.084515</td>\n",
       "      <td>0.084515</td>\n",
       "      <td>0.084515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    479532    144749    174171    832412    828689    994433    1005907  \\\n",
       "0  0.338062  0.169031  0.169031  0.169031  0.169031  0.169031  0.169031   \n",
       "\n",
       "    170062    675997    959146    784967    637271    51251     134503   \\\n",
       "0  0.169031  0.169031  0.084515  0.084515  0.084515  0.084515  0.084515   \n",
       "\n",
       "    947540   \n",
       "0  0.084515  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam  = pd.DataFrame(hvec.transform([spam]).todense())\n",
    "\n",
    "df_spam.transpose().sort_values(0, ascending=False).head(15).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11e7279d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAFVCAYAAADPM8ekAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1039919d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(df_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>174171</th>\n",
       "      <th>479532</th>\n",
       "      <th>832412</th>\n",
       "      <th>195890</th>\n",
       "      <th>384242</th>\n",
       "      <th>170543</th>\n",
       "      <th>308947</th>\n",
       "      <th>393419</th>\n",
       "      <th>512176</th>\n",
       "      <th>792260</th>\n",
       "      <th>352275</th>\n",
       "      <th>784967</th>\n",
       "      <th>127771</th>\n",
       "      <th>667487</th>\n",
       "      <th>403377</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.413803</td>\n",
       "      <td>0.331042</td>\n",
       "      <td>0.331042</td>\n",
       "      <td>0.165521</td>\n",
       "      <td>0.165521</td>\n",
       "      <td>0.165521</td>\n",
       "      <td>0.165521</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.082761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     174171    479532    832412    195890    384242    170543    308947  \\\n",
       "0  0.413803  0.331042  0.331042  0.165521  0.165521  0.165521  0.165521   \n",
       "\n",
       "     393419    512176    792260    352275    784967    127771    667487  \\\n",
       "0  0.082761  0.082761  0.082761  0.082761  0.082761  0.082761  0.082761   \n",
       "\n",
       "     403377  \n",
       "0  0.082761  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ham  = pd.DataFrame(hvec1.transform([ham]).todense())\n",
    "\n",
    "df_ham.transpose().sort_values(0, ascending=False).head(15).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  0\n",
       "0  0  0\n",
       "1  0  0\n",
       "2  0  0\n",
       "3  0  0\n",
       "4  0  0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woah = pd.concat([df_ham, df_spam]).sort_values(0, ascending=False).transpose()\n",
    "woah.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Then if we want to see it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Notice we lost our feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## We still haven't talked about all those problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bag of word approaches like the one outlined before completely ignore the structure of a sentence, they merely assess presence of specific words or word combinations.\n",
    "\n",
    "Additionally, the same word can have multiple meanings in different contexts. Consider for example the following two sentencs:\n",
    "\n",
    "- Malcom received his first **estimate** yesterday.\n",
    "- I **estimate** there's more to come in NLP.\n",
    "\n",
    "How do we teach a computer to disambiguate? How do we even deal with sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_Segmentation_ is a technique to identify sentences within a body of text. Language is not a continuous uninterrupted stream of words: punctuation serves as a guide to group together words that convey meaning when contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "\n",
    "easy_split_text = [\"I went to the zoo today.\",\n",
    "                   \"What do you think of that?\",\n",
    "                   \"I bet you hate it!\",\n",
    "                   \"Or maybe you don't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " ' What do you think of that?',\n",
       " ' I bet you hate it!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_sentencer(text):\n",
    "    '''take a string called `text` and return\n",
    "    a list of strings, each containing a sentence'''\n",
    "    \n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "    return sentences\n",
    "\n",
    "simple_sentencer(easy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Not the best solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducing NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://covers.oreilly.com/images/9780596516499/cat.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://covers.oreilly.com/images/9780596516499/cat.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLTK - the Natural Language Toolkit has a suite of tools to help with exactly these kinds of problems.<br><br>It also comes with a number of public domain corpora that you can use as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's do some segmentation with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " 'What do you think of that?',\n",
       " 'I bet you hate it!',\n",
       " \"Or maybe you don't\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sent_detector = PunktSentenceTokenizer()\n",
    "sent_detector.sentences_from_text(easy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:<br><br> Does NLTK offer other Tokenizers? Use nltk.download() to explore the available packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "_Normalization_ is when slightly different version of a word exist. For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question: What are other common scenarios where text could need normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Person titles (Mr. MR. DR etc.)\n",
    "- Dates (10/03, March 10 etc.)\n",
    "- Numbers\n",
    "- Plurals\n",
    "- Verb conjugations\n",
    "- Slang\n",
    "- Sms abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It would be wrong to consider the words \"MR.\" and \"mr\" to be different features, thus we need a technique to normalize words to a common root. This technique is called _Stemming_.\n",
    "\n",
    "- Science, Scientist => Scien\n",
    "- Swimming, Swimmer, Swim => Swim\n",
    "\n",
    "As we did above we could define a Stemmer based on rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    '''rules-based stemming of a bunch of tokens'''\n",
    "    \n",
    "    new_bag = []\n",
    "    for token in tokens:\n",
    "        # define rules here\n",
    "        if token.endswith('s'):\n",
    "            new_bag.append(token[:-1])\n",
    "        elif token.endswith('er'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('tion'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('tist'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('ce'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('ing'):\n",
    "            new_bag.append(token[:-2])\n",
    "        else:\n",
    "            new_bag.append(token)\n",
    "\n",
    "    return new_bag\n",
    "\n",
    "stem(['Science', 'Scientist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Again, luckily for us, NLTK contains several robust stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print stemmer.stem('Swimmed')\n",
    "print stemmer.stem('Swimming')\n",
    "\n",
    "print(stemmer.stem('Falling, Forking, Longing, Swimificationalize'))\n",
    "print(stemmer.stem('Longing'))\n",
    "print(stemmer.stem('Falling'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:<br><br> There are other stemmers available in NLTK. Let's split the class in 2 teams and have a look at [this article](https://www.elastic.co/guide/en/elasticsearch/guide/current/choosing-a-stemmer.html). One team will focus on the pros of the Porter Stemmer, the other team will focus on the pros of the Snowball stemmer. You have 5 minutes to read, then each side will have 2 minutes to convince the other side about their stemmer.<br><br>Bonus: Find another method that works in a similar way, but is more nuanced with its normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question: We talked about the need to trim features, what might be some words that we could dispense with in our modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We can remove these with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "sentence = \"this is a foo bar sentence\"\n",
    "print [i for i in sentence.split() if i not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(PATH + 'diagram.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Except now it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(PATH + 'tree.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each word has a specific role in a sentence (Verb, Noun etc.) Parts-of-speech tagging (POS) is a feature extraction technique that attaches a tag to each word in the sentence, in order to provide a more precise context for further analysis. This is often a resource intensive process, but it can sometimes improve the accuracy or our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pos_tag(tok.tokenize(\"today is a great day to learn nlp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Use NLTK to get the POS for spam or ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok_ham = WordPunctTokenizer()\n",
    "pos_tag(tok_ham.tokenize(ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question: Suppose now we wanted to match documents to search terms using only the words on the page. How could we match a search phrase like 'kittens' to the best document for that term?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term frequency - Inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "If a word is rare across all documents, but frequent in a given document, we can say that that document\n",
    "is likely to be highly relevant to that subject.\n",
    "\n",
    "Let's see how it is calculated.\n",
    "\n",
    "Term frequency tf is the frequency of a certain term in a document:\n",
    "$$\n",
    "\\mathrm{tf}(t,d) = \\frac{N_\\text{term}}{N_\\text{terms in Document}}\n",
    "$$\n",
    "Inverse document frequency is defined as the frequency of documents that contain that term over the whole corpus.\n",
    "$$\n",
    "\\mathrm{idf}(t, D) = \\log\\frac{N_\\text{Documents}}{N_\\text{Documents that contain term}}\n",
    "$$\n",
    "\n",
    "Term frequency - Inverse Document Frequency is calculated as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n",
    "\n",
    "This enhances terms that are highly specific of a particular document, while suppressing terms that are common to most documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit Learn has a TFIDF vectorizer that works similarly to the other vectorizers we used previously. Notice that now we can also eliminate stop words to improve our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:<br><br> As you did above, import and initialize the TfidfVectorizer, then fit the spam and ham data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = TfidfVectorizer(stop_words='english')\n",
    "df.fit([spam, ham])\n",
    "\n",
    "df  = pd.DataFrame(df.transform([spam, ham]).todense(),\n",
    "             columns=df.get_feature_names(), index=['spam', 'ham'])\n",
    "\n",
    "df_ham = df.transpose().sort_values('ham', ascending=False).head(15).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>750</th>\n",
       "      <th>86</th>\n",
       "      <th>ago</th>\n",
       "      <th>application</th>\n",
       "      <th>assistance</th>\n",
       "      <th>attached</th>\n",
       "      <th>best</th>\n",
       "      <th>board</th>\n",
       "      <th>...</th>\n",
       "      <th>seven</th>\n",
       "      <th>site</th>\n",
       "      <th>smith</th>\n",
       "      <th>sum</th>\n",
       "      <th>thousand</th>\n",
       "      <th>time</th>\n",
       "      <th>valery</th>\n",
       "      <th>week</th>\n",
       "      <th>writing</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00       000       750        86       ago  application  \\\n",
       "spam  0.143564  0.143564  0.143564  0.143564  0.143564     0.000000   \n",
       "ham   0.000000  0.000000  0.000000  0.000000  0.000000     0.155195   \n",
       "\n",
       "      assistance  attached      best     board    ...        seven      site  \\\n",
       "spam    0.000000  0.000000  0.000000  0.143564    ...     0.143564  0.000000   \n",
       "ham     0.155195  0.155195  0.155195  0.000000    ...     0.000000  0.155195   \n",
       "\n",
       "         smith       sum  thousand      time    valery      week   writing  \\\n",
       "spam  0.000000  0.143564  0.143564  0.000000  0.143564  0.143564  0.000000   \n",
       "ham   0.155195  0.000000  0.000000  0.155195  0.000000  0.000000  0.155195   \n",
       "\n",
       "         years  \n",
       "spam  0.287128  \n",
       "ham   0.000000  \n",
       "\n",
       "[2 rows x 69 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv_ham = TfidfVectorizer(stop_words='english')\n",
    "tv_ham.fit([ham])\n",
    "\n",
    "tv_ham  = pd.DataFrame(tv_ham.transform([ham]).todense(),\n",
    "             columns=tv_ham.get_feature_names())\n",
    "\n",
    "df_ham_only = tv_ham.transpose().sort_values(0, ascending=False).head(15).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'unicode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-478a874f595a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ham_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'unicode'"
     ]
    }
   ],
   "source": [
    "print(sum(df_ham_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sklearn.feature_extraction.text.TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now we can see the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question: What is TF-IDF? describe with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this lesson we learned about Natural Language Processing and about two very powerful toolkits:\n",
    "- Scikit Learn Feature Extraction for Text\n",
    "- Natural Language Tool Kit\n",
    "\n",
    "And we learned several techniques for working with language-based data:\n",
    "- Sentence segmentation\n",
    "- Stop word removal\n",
    "- Stemming and Lemmatization\n",
    "- POS tagging\n",
    "- Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Count Vectorizer Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [Choosing a Stemmer](https://www.elastic.co/guide/en/elasticsearch/guide/current/choosing-a-stemmer.html)\n",
    "- [Feature Hashing](https://en.wikipedia.org/wiki/Feature_hashing)\n",
    "- [Term Frequency Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- [TFIDF Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
