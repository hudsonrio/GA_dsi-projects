{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "Once you've chosen your scenario, download the data from [the Iowa website](https://data.iowa.gov/Economy/Iowa-Liquor-Sales/m3tr-qhgy) in csv format. Start by loading the data with pandas. You may need to parse the date columns appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>Reportdata number 1</th>\n",
       "      <th>Reportdata link 1</th>\n",
       "      <th>Reportdata link 1_link</th>\n",
       "      <th>Reportdata number 2</th>\n",
       "      <th>Reportdata link 2</th>\n",
       "      <th>Reportdata link 2_link</th>\n",
       "      <th>Reportdata number 3</th>\n",
       "      <th>Reportdata number 4</th>\n",
       "      <th>Reportdata number 5</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://zipatlas.com/us/ia/zip-code-comparison/...</td>\n",
       "      <td>101</td>\n",
       "      <td>50009</td>\n",
       "      <td>http://zipatlas.com/us/ia/altoona/zip-50009.htm</td>\n",
       "      <td>41.640625, -93.458835</td>\n",
       "      <td>Altoona</td>\n",
       "      <td>http://zipatlas.com/us/ia/altoona.htm</td>\n",
       "      <td>12,008</td>\n",
       "      <td>406.63</td>\n",
       "      <td>#8,763</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://zipatlas.com/us/ia/zip-code-comparison/...</td>\n",
       "      <td>102</td>\n",
       "      <td>51451</td>\n",
       "      <td>http://zipatlas.com/us/ia/lanesboro/zip-51451.htm</td>\n",
       "      <td>42.184080, -94.695158</td>\n",
       "      <td>Lanesboro</td>\n",
       "      <td>http://zipatlas.com/us/ia/lanesboro.htm</td>\n",
       "      <td>148</td>\n",
       "      <td>401.19</td>\n",
       "      <td>#8,801</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://zipatlas.com/us/ia/zip-code-comparison/...</td>\n",
       "      <td>103</td>\n",
       "      <td>52002</td>\n",
       "      <td>http://zipatlas.com/us/ia/dubuque/zip-52002.htm</td>\n",
       "      <td>42.521344, -90.776310</td>\n",
       "      <td>Dubuque</td>\n",
       "      <td>http://zipatlas.com/us/ia/dubuque.htm</td>\n",
       "      <td>11,539</td>\n",
       "      <td>400.73</td>\n",
       "      <td>#8,806</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://zipatlas.com/us/ia/zip-code-comparison/...</td>\n",
       "      <td>104</td>\n",
       "      <td>50706</td>\n",
       "      <td>http://zipatlas.com/us/ia/waterloo/zip-50706.htm</td>\n",
       "      <td>42.408728, -92.258971</td>\n",
       "      <td>Waterloo</td>\n",
       "      <td>http://zipatlas.com/us/ia/waterloo.htm</td>\n",
       "      <td>1,035</td>\n",
       "      <td>395.19</td>\n",
       "      <td>#8,843</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://zipatlas.com/us/ia/zip-code-comparison/...</td>\n",
       "      <td>105</td>\n",
       "      <td>51355</td>\n",
       "      <td>http://zipatlas.com/us/ia/okoboji/zip-51355.htm</td>\n",
       "      <td>43.388189, -95.136678</td>\n",
       "      <td>Okoboji</td>\n",
       "      <td>http://zipatlas.com/us/ia/okoboji.htm</td>\n",
       "      <td>825</td>\n",
       "      <td>388.07</td>\n",
       "      <td>#8,902</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  Reportdata number 1  \\\n",
       "0  http://zipatlas.com/us/ia/zip-code-comparison/...                  101   \n",
       "1  http://zipatlas.com/us/ia/zip-code-comparison/...                  102   \n",
       "2  http://zipatlas.com/us/ia/zip-code-comparison/...                  103   \n",
       "3  http://zipatlas.com/us/ia/zip-code-comparison/...                  104   \n",
       "4  http://zipatlas.com/us/ia/zip-code-comparison/...                  105   \n",
       "\n",
       "   Reportdata link 1                             Reportdata link 1_link  \\\n",
       "0              50009    http://zipatlas.com/us/ia/altoona/zip-50009.htm   \n",
       "1              51451  http://zipatlas.com/us/ia/lanesboro/zip-51451.htm   \n",
       "2              52002    http://zipatlas.com/us/ia/dubuque/zip-52002.htm   \n",
       "3              50706   http://zipatlas.com/us/ia/waterloo/zip-50706.htm   \n",
       "4              51355    http://zipatlas.com/us/ia/okoboji/zip-51355.htm   \n",
       "\n",
       "     Reportdata number 2 Reportdata link 2  \\\n",
       "0  41.640625, -93.458835           Altoona   \n",
       "1  42.184080, -94.695158         Lanesboro   \n",
       "2  42.521344, -90.776310           Dubuque   \n",
       "3  42.408728, -92.258971          Waterloo   \n",
       "4  43.388189, -95.136678           Okoboji   \n",
       "\n",
       "                    Reportdata link 2_link Reportdata number 3  \\\n",
       "0    http://zipatlas.com/us/ia/altoona.htm              12,008   \n",
       "1  http://zipatlas.com/us/ia/lanesboro.htm                 148   \n",
       "2    http://zipatlas.com/us/ia/dubuque.htm              11,539   \n",
       "3   http://zipatlas.com/us/ia/waterloo.htm               1,035   \n",
       "4    http://zipatlas.com/us/ia/okoboji.htm                 825   \n",
       "\n",
       "  Reportdata number 4 Reportdata number 5  Unnamed: 10  \n",
       "0              406.63              #8,763          407  \n",
       "1              401.19              #8,801          401  \n",
       "2              400.73              #8,806          401  \n",
       "3              395.19              #8,843          395  \n",
       "4              388.07              #8,902          388  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "import os\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# iowa_data = pd.read_csv('/Users/HudsonCavanagh/Documents/Iowa_Liquor_Sales.csv') -- this is where need to update with full dataset (eventually)\n",
    "iowa_data = pd.read_csv('/Users/HudsonCavanagh/Documents/Iowa_Liquor_sales_sample_10pct.csv')\n",
    "ia = pd.DataFrame(iowa_data) \n",
    "ia.dropna(axis=0, inplace=True)\n",
    "\n",
    "iowa_zip_data = pd.read_csv('/Users/HudsonCavanagh/dsi-projects/projects/3rd - IA Liquor/iowa_zip_pop.csv')\n",
    "ia_zip_pop = pd.DataFrame(iowa_zip_data) \n",
    "\n",
    "\n",
    "\n",
    "ia['Date'] = pd.to_datetime(ia['Date'], infer_datetime_format=True)\n",
    "\n",
    "ia_zip_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean IA ZIP POP HERE\n",
    "\n",
    "# ia_zip_pop = ia_zip_pop.loc['Reportdata link 1', 'Reportdata number 2', 'Reportdata link 2', 'Reportdata number 4']\n",
    "##ORIGINAL VERSION:\n",
    "# ia_zip_pop = ia_zip_pop.iloc[:,(2,4,5,8)]\n",
    "# column_iazip = ['zip', 'lat_long', 'city', 'pop_dense_heads_sqm']\n",
    "\n",
    "#REVISED FOR MERGING\n",
    "\n",
    "ia_zip_pop.dropna(axis=0, inplace=True)\n",
    "ia_zip_pop = ia_zip_pop.iloc[:,(2,10)]\n",
    "column_iazip = ['zip', 'pop_dense_heads_sqm']\n",
    "ia_zip_pop.columns = column_iazip\n",
    "# ia_zip_pop.dropna(inplace=True)\n",
    "ia_zip_pop['zip'] = ia_zip_pop['zip'].apply(lambda x: str(x))\n",
    "ia_zip_pop['pop_dense_heads_sqm'] = ia_zip_pop['pop_dense_heads_sqm'].apply(lambda x: float(x)) \n",
    "#could not get the above to read commas in numbers over 1k with float or int, did conversion in google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>pop_dense_heads_sqm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50009</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51451</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52002</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50706</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51355</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     zip  pop_dense_heads_sqm\n",
       "0  50009                  407\n",
       "1  51451                  401\n",
       "2  52002                  401\n",
       "3  50706                  395\n",
       "4  51355                  388"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia_zip_pop.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269258"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ia.rename(columns={'Date': 'date', 'Store Number': 'store_num', 'City': 'city', 'Zip Code': 'zip', 'County Number': 'county_num', 'County': 'county_name', 'Category': 'cat', 'Category Name': 'cat_name'}, inplace=True)\n",
    "ia.rename(columns={'Vendor Number': 'vend_id', 'Item Number': 'item_id', 'Item Description': 'item', 'Bottle Volume (ml)': 'vol_per_bottle_ml', 'State Bottle Cost': 'bottle_cost', 'State Bottle Retail': 'retail_unit_rev', 'Bottles Sold': 'bottles_sold'}, inplace=True)\n",
    "ia.rename(columns={'Sale (Dollars)': 'trans_revenue', 'Volume Sold (Liters)': 'vol_sold_liters'}, inplace=True)\n",
    "ia = ia.iloc[:,0:17]\n",
    "\n",
    "len(ia['store_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CONVERT DATATYPES\n",
    "\n",
    "# ia['date'].value_counts  #dtype time 64\n",
    "# ia['store_num'].value_counts  #int, should be string\n",
    "ia['store_num'] = ia['store_num'].apply(lambda x: str(x))\n",
    "# ia['city'].value_counts #object, ok\n",
    "# ia['zip'].value_counts #object, ok\n",
    "# ia['county_num'].value_counts #float, should be object\n",
    "ia['county_num'] = ia['county_num'].apply(lambda x: str(x))\n",
    "# ia['county_name'].value_counts #object, ok\n",
    "# ia['cat'].value_counts #int, should be obj\n",
    "                                          \n",
    "ia['cat'] = ia['cat'].apply(lambda x: str(x))\n",
    "                                          \n",
    "                                 \n",
    "# ia['cat_name'].value_counts #str\n",
    "ia['vend_id'] = ia['vend_id'].apply(lambda x: str(x))\n",
    "ia['item_id'] = ia['item_id'].apply(lambda x: str(x))\n",
    "# ia['item'].value_counts #str\n",
    "ia['liters_per_bottle'] = (ia['vol_per_bottle_ml'].apply(lambda x: float(x)/1000))\n",
    "ia['bottles_sold'] = (ia['bottles_sold'].apply(lambda x: float(x)))\n",
    "\n",
    "##THIS DATA CLEAN, JUST NEED TO CONVERT DOLLAR COLUMNS\n",
    "\n",
    "ia['bottle_cost'] = ia['bottle_cost'].apply(lambda x: x.strip('$'))\n",
    "ia['bottle_cost'] = ia['bottle_cost'].apply(lambda x: float(x))\n",
    "\n",
    "ia['retail_unit_rev'] = ia['retail_unit_rev'].apply(lambda x: x.strip('$'))\n",
    "ia['retail_unit_rev'] = ia['retail_unit_rev'].apply(lambda x: float(x))\n",
    "\n",
    "ia['trans_revenue'] = ia['trans_revenue'].apply(lambda x: x.strip('$'))\n",
    "ia['trans_revenue'] = ia['trans_revenue'].apply(lambda x: float(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add calculated features\n",
    "\n",
    "ia['profit'] = (ia['retail_unit_rev']-ia['bottle_cost'])*ia['bottles_sold']\n",
    "ia['profit_per_L'] = ia['profit']/ia['liters_per_bottle']\n",
    "ia['profit_margin'] = ia['profit']/ia['trans_revenue']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ia_pop = pd.merge(ia, ia_zip_pop, how='left', on='zip', sort=False)\n",
    "g = ia_pop.set_index('zip')\n",
    "\n",
    "\n",
    "def zipper(values):\n",
    "    works = []\n",
    "    issues = []\n",
    "    \n",
    "    for val in values:\n",
    "        if '-' in val:\n",
    "            issues.append(val)\n",
    "        elif str(int(val)) == val:\n",
    "            works.append(val)\n",
    "        else:\n",
    "            issues.append(val)\n",
    "    return works, issues\n",
    "        \n",
    "works, issues = zipper(g.index) \n",
    "g = g.loc[works]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#WORKING SPLITTING CELL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g['year'] = g.date.dt.year\n",
    "g['quarters'] = g.date.dt.quarter\n",
    "g_2015 = g[g.year == 2015]\n",
    "q1 = g_2015[g_2015.quarters == 1]\n",
    "q2 = g_2015[g_2015.quarters == 2]\n",
    "q3 = g_2015[g_2015.quarters == 3]\n",
    "q4 = g_2015[g_2015.quarters == 4]\n",
    "q234 = g_2015[g_2015.quarters == [2 or 3 or 4]]\n",
    "q5 = g[g.year == 2016]\n",
    "\n",
    "q1_X = q1.drop('trans_revenue', 1)\n",
    "q1_X['constant'] = 1\n",
    "\n",
    "#COULD ADD BACK CAT, VEND categoricals\n",
    "q1_X = q1_X[['vol_sold_liters', 'profit', 'profit_per_L', 'profit_margin','pop_dense_heads_sqm', 'constant']]\n",
    "q234_y = q234['trans_revenue']\n",
    "q234_Y = pd.DataFrame(q234_y)\n",
    "\n",
    "q1_X_trim = pd.merge(q1_X, q234_Y, how='left', left_index=1, right_index=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # q234_Y = q234_Y['trans_revenue']\n",
    "\n",
    "# # merge(left, right, how='inner', on=None, left_on=None, right_on=None,\n",
    "# #       left_index=False, right_index=False\n",
    "\n",
    "\n",
    "# # g_date_storeQ_y = g_date_storeQ['trans_revenue']\n",
    "# print(len(q1_X_trim), len(q234_y))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(q1_X), len(q1_X_trim)len(q234_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(q1_X, q234_y, test_size=.33)\n",
    "\n",
    "\n",
    "print \"       X Shape  Y Shape\"\n",
    "print \"Train\", X_train.shape, y_train.shape\n",
    "print \"Test \", X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # g = ia_pop.groupby(pd.TimeGrouper('M', closed = 'left')).aggregate(numpy.sum)\n",
    "\n",
    "# # store_ia = g.groupby(['store_num']) #should already be grouped by month at this point\n",
    "# # store_ia.head()\n",
    "\n",
    "# g_date_storeQ = ia_pop.set_index('date').groupby(pd.TimeGrouper('Q')).apply(lambda x: x.groupby('store_num').sum())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # use if STATEMENTS TO FILTER EACH OF THESE QUARTERS MANUALLY ACCORDING TO DATE\n",
    "\n",
    "\n",
    "# g_date_storeM = ia_pop.set_index('date').groupby(pd.TimeGrouper('M')).apply(lambda x: x.groupby('store_num').sum())\n",
    "# #train-test-split this \n",
    "\n",
    "# # g_store = ia_pop.set_index('store_num')\n",
    "# # grouped_store = g_store.groupby(lambda x: x.month)\n",
    "# # store_sales = grouped_store.aggregate(np.sum)\n",
    "\n",
    "\n",
    "# # g_store.head()\n",
    "\n",
    "# g_date_storeQ_X = g_date_storeQ.drop('trans_revenue', 1)\n",
    "# g_date_storeQ_y = g_date_storeQ['trans_revenue']\n",
    "\n",
    "\n",
    "# Q1_2015_y = g_date_storeQ_y.loc['2015-03-31']\n",
    "\n",
    "# Q2_2015_y = g_date_storeQ_y.loc['2015-06-30']\n",
    "\n",
    "\n",
    "# Q3_2015_y = g_date_storeQ_y.loc['2015-09-30']\n",
    "# Q4_2015_y = g_date_storeQ_y.loc['2015-12-31']\n",
    "# Q1_2016_y = g_date_storeQ_y.loc['2016-03-31']\n",
    "\n",
    "# Q1_2015_X = g_date_storeQ_X.loc['2015-03-31']\n",
    "# Q2_2015_X = g_date_storeQ_X.loc['2015-06-30']\n",
    "# Q3_2015_X = g_date_storeQ_X.loc['2015-09-30']\n",
    "# Q4_2015_X = g_date_storeQ_X.loc['2015-12-31']\n",
    "# Q1_2016_X = g_date_storeQ_X.loc['2016-03-31']\n",
    "\n",
    "# # g_date_storeQ_X.head()\n",
    "\n",
    "# # frames = [Q2_2015_y, Q3_2015_y, Q4_2015_y]\n",
    "# # Q23_2015_Y = pd.merge(Q2_2015_y, Q3_2015_y, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# print(g)\n",
    "\n",
    "\n",
    "\n",
    "# #SAM: train on Q1 data in order to evaluate Q2,3,4 in 2015, \n",
    "# #then use the model used to predict Q2,3,4 in 2015 on the same in 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "\n",
    "Perform some exploratory statistical analysis and make some plots, such as histograms of transaction totals, bottles sold, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ia_pop['store_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record your findings\n",
    "\n",
    "Be sure to write out anything observations from your exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine the data\n",
    "Now you are ready to compute the variables you will use for your regression from the data. For example, you may want to\n",
    "compute total sales per store from Jan to March of 2015, mean price per bottle, etc. Refer to the readme for more ideas appropriate to your scenario.\n",
    "\n",
    "Pandas is your friend for this task. Take a look at the operations [here](http://pandas.pydata.org/pandas-docs/stable/groupby.html) for ideas on how to make the best use of pandas and feel free to search for blog and Stack Overflow posts to help you group data by certain variables and compute sums, means, etc. You may find it useful to create a new data frame to house this summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q1_2015_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q234_2015_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine the data\n",
    "Look for any statistical relationships, correlations, or other relevant properties of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAKE TONS OF PLOTS, seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your models\n",
    "\n",
    "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "X = ia[[\"RM, other col, other col, other col\"]]\n",
    "y = targets[\"MEDV\"]\n",
    "\n",
    "model = lm.fit(X, y)\n",
    "predictions = lm.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How is it performing? Plot the model's predictions against actual values\n",
    "# s = s: size in points, c = color, zorder = layer order\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.scatter(predictions, y, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Predicted Values from RM\")\n",
    "plt.ylabel(\"Actual Values MEDV\")\n",
    "plt.show()\n",
    "print \"MSE:\", mean_squared_error(y, predictions)\n",
    "\n",
    "cross_val_score(lr, X, y, n_jobs=1, cv=5).mean() #multiple jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the residuals\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.scatter(y, y - predictions, c = 'b', marker = '+') # Look directly at the residuals\n",
    "plt.axhline(0, color='r') #from http://localhost:8888/notebooks/week03/W3%20L1.3_scikit-modeling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OLS\n",
    "\n",
    "# Note the difference in argument order\n",
    "OLSmodel = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Plot the model\n",
    "plt.scatter(predictions, y, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Predicted Values from RM\")\n",
    "plt.ylabel(\"Actual Values MEDV\")\n",
    "plt.show()\n",
    "print \"MSE:\", mean_squared_error(y, predictions)\n",
    "\n",
    "cross_val_score(lr, X, y, n_jobs=1, cv=5).mean() #multiple jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(OLSmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standardized Ridge Model\n",
    "\n",
    "rcv = linear_model.RidgeCV(alphas=\n",
    "                           (.001, .001, .01, .1, .5, 1, 5, 10),\n",
    "                           store_cv_values=True)\n",
    "rcv_model = rcv.fit(X_train, y_train)\n",
    "y_predicted = rcv_model.predict(X_test)\n",
    "rcv_r2 =  r2_score(y_true=y_test, y_pred=y_predicted)\n",
    "rcv_r2\n",
    "\n",
    "plt.scatter(y_test, y_predicted) #compare the hold-out observed y values to the predictions (made with model from test data)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "\n",
    "print \"Score:\", rcv_model.score(X_test, y_test)\n",
    "rcv_model.cv_values_.mean()\n",
    "\n",
    "rcv_model.alpha_ #this returns optimal alpha for dataset\n",
    "\n",
    "cross_val_score(lr, X, y, n_jobs=1, cv=5).mean() #multiple jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standardized lasso model\n",
    "\n",
    "lcv = linear_model.LassoCV()\n",
    "lcv_model = lcv.fit(X_train, y_train)\n",
    "y_lasso_predicted = lcv_model.predict(X_test)\n",
    "lcv_r2 =  r2_score(y_true=y_test, y_pred=y_predicted)\n",
    "lcv_r2\n",
    "\n",
    "plt.scatter(y_test, y_lasso_predicted) #compare the hold-out observed y values to the predictions (made with model from test data)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Lasso Predictions\")\n",
    "\n",
    "print \"Score:\", lcv_model.score(X_test, y_test)\n",
    "\n",
    "print(lcv_model.alpha_, #this returns optimal alpha for dataset\n",
    "abs(lcv_model.coef_))\n",
    "\n",
    "final_predicts = ia.predict(modelname, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iterate through r2 or mse a bunch\n",
    "\n",
    "for i in range(1000):\n",
    "    cross_val_list = []\n",
    "    n = cross_val_score(lasso, X, y, n_jobs=1, cv=5, scoring='r2').mean()\n",
    "    cross_val_list.append(n)\n",
    "print(np.mean(cross_val_list))\n",
    "\n",
    "\n",
    "# cross_val_score(lr, X, y, n_jobs=1, cv=5,\n",
    "#                 scoring='mean_squared_error').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample code for cross_validated output\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dd, dy, test_size=.4)\n",
    "\n",
    "lcv_model = lcv.fit(X_train, y_train)\n",
    "lcv_pred = lcv.predict(X_test)\n",
    "lasso_r2 =  r2_score(y_true=y_test, y_pred=lcv_pred)\n",
    "print(\"R sq for Lasso Reg is:\", lasso_r2)\n",
    "\n",
    "print(\"cross validated r^2:\", np.mean(cross_val_score(lcv_model, X_test, y_test, scoring='r2', cv=5)))\n",
    "print(\"r^2 w/o cross-validation\", (lcv_model.score(X_test,y_test)))\n",
    "print(\"cross validated MSE (sign flipped):\", -np.mean(cross_val_score(lcv_model, X_test, y_test, scoring='mean_squared_error', cv=5)))\n",
    "print(\"MSE w/o cross-validation\", (lcv_model.score(X_test,y_test)))\n",
    "\n",
    "# mse\n",
    "# bar\n",
    "# bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###LOSS FUNCTIONS\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print \"RMSE:\", mean_squared_error(ys, predictions)\n",
    "print \"MAE:\", mean_absolute_error(ys, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#QUANT REG\n",
    "\n",
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=[\"x\", \"y\"])\n",
    "df.columns = [\"x\", \"y\"]\n",
    "mod = smf.quantreg('y ~ x', df)\n",
    "res = mod.fit(q=.5)\n",
    "print(res.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HELPFUL FUNCTIONS\n",
    "\n",
    "# Gradient Descent/ Optimizing Functions:\n",
    "    \n",
    "    \n",
    "def mean_squared_error(y_true, x, beta0, beta1):\n",
    "    y_pred = beta0 + x * beta1\n",
    "    mean_sq_err = np.mean((y_true - y_pred)**2)\n",
    "    return mean_sq_err\n",
    "\n",
    "def gradient_update(y, x, beta0, beta1, step_size):\n",
    "    \n",
    "    beta0_gradient = 0\n",
    "    beta1_gradient = 0\n",
    "    \n",
    "    N = float(len(y))\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        \n",
    "        # add to the beta0 gradient for each x,y using the partial derivative with respect to beta0\n",
    "        beta0_gradient += (2./N * -1 * (y[i] - (beta0 + beta1*x[i])))\n",
    "        \n",
    "        # add to the beta1 gradient for each x,y using the partial derivative with respect to beta1\n",
    "        beta1_gradient += (2./N * -1 * x[i] * (y[i] - (beta0 + beta1*x[i])))\n",
    "        \n",
    "    # update beta0 and beta1:\n",
    "    beta0 = beta0 - (step_size * beta0_gradient) #subtracting because we want to move in the opposite direction of the gradient\n",
    "    #this is because we want to minimize function\n",
    "    beta1 = beta1 - (step_size * beta1_gradient)\n",
    "    \n",
    "    return [beta0, beta1]\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent_iterator(y, x, beta0, beta1, step_size=.0001, iterations=500):\n",
    "    \n",
    "    mean_squared_errors = []\n",
    "    mean_squared_errors.append(mean_squared_error(y, x, beta0, beta1))\n",
    "    \n",
    "    beta0s = [beta0]\n",
    "    beta1s = [beta1]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        [beta0, beta1] = gradient_update(y, x, beta0, beta1, step_size)\n",
    "        mean_squared_errors.append(mean_squared_error(y, x, beta0, beta1))\n",
    "        beta0s.append(beta0)\n",
    "        beta1s.append(beta1)\n",
    "        \n",
    "    return [mean_squared_errors, beta0s, beta1s]\n",
    "\n",
    "\n",
    "\n",
    "# format for runnning the above 3 functions:\n",
    "    \n",
    "x = np.random.random_sample(100)*100\n",
    "y = x + np.random.normal(np.random.normal(0,15), 30, size=100) + 100\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(x, y, s=70, c='steelblue')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "### Plotting functions from earlier (nonessential)\n",
    "\n",
    "\n",
    "\n",
    "def plot_regression(x, y, model):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    axes = plt.gca()\n",
    "    \n",
    "    intercept = model.params[0]\n",
    "    slope = model.params[1]\n",
    "\n",
    "    for x_, y_ in zip(x, y):    \n",
    "        plt.plot((x_, x_), (y_, x_*slope + intercept),\n",
    "                 'k-', ls='dashed', lw=1)\n",
    "        \n",
    "    plt.scatter(x, y, s=70, c='steelblue')\n",
    "    \n",
    "    x_points = np.linspace(axes.get_xlim()[0], axes.get_xlim()[1], 100)\n",
    "    \n",
    "    regline_x = x_points\n",
    "    regline_y = x_points*slope + intercept\n",
    "\n",
    "    plt.plot(regline_x, regline_y, c='darkred', lw=3.5)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_leastsq_loss(model):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    \n",
    "    resids = model.resid\n",
    "    \n",
    "    resid_lim = np.max([abs(np.min(resids)), abs(np.max(resids))]) + 1\n",
    "    \n",
    "    resid_points = np.linspace(-1*resid_lim, resid_lim, 200)\n",
    "    \n",
    "    for r in resids:\n",
    "        plt.plot((r, r), (0, r**2), 'k-', ls='dashed', lw=2)\n",
    "        \n",
    "    plt.plot(resid_points, resid_points**2, c='gold', alpha=0.7)\n",
    "    \n",
    "\n",
    "def plot_residuals_ladloss(model):\n",
    "    \n",
    "    resids = model.resid\n",
    "    \n",
    "    resid_lim = np.max([abs(np.min(resids)), abs(np.max(resids))]) + 1\n",
    "    \n",
    "    resid_points = np.linspace(-1*resid_lim, resid_lim, 200)\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    \n",
    "    for r in resids:\n",
    "        \n",
    "        plt.plot((r, r), (0, abs(r)), 'k-', ls='dashed', lw=1)\n",
    "        \n",
    "    plt.plot(resid_points, np.abs(resid_points), c='gold', alpha=0.7)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "##REMINDER OF NEXT STEPS:\n",
    "#from http://localhost:8888/notebooks/week03/loss-functions-regression-metrics-practice.ipynb\n",
    "# 4: Choose a continuous response variable and predictor variable from the dataset\n",
    "# 5: Choose a small subset of the predictor and response variables you chose\n",
    "# 6. Build a least squares regression model predicting your response from your predictors\n",
    "plot_regression(x, y, model)\n",
    "7. Plot the least squares regression\n",
    "8. Build a least absolute deviation quantreg model on the same sample\n",
    "Plot the LAD regression\n",
    "\n",
    "10. Calculate the RMSE and the MAE between you response and predicted response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUll run-through of gradient descent iteration (redundant with some code above):\n",
    "    \n",
    "def func(x):    #this needs to be modified to whatever the specific function is\n",
    "    if x <= 1:\n",
    "        return 2 * x * x\n",
    "    return 2\n",
    "\n",
    "def gradient(x): #this needs to be the derivative of the func\n",
    "    if x <= 1:\n",
    "        return 4 * x\n",
    "    return 0\n",
    "\n",
    "def gradient_descent(x, l=0.1):\n",
    "    vector = np.array(x)\n",
    "    return vector - l * np.array(gradient(x))\n",
    "\n",
    "\n",
    "def iterate(x0, n=10):\n",
    "    xs = [x0]\n",
    "    ys = [func(x0)]\n",
    "    for i in range(n):\n",
    "        x = gradient_descent(xs[-1], l=0.1)\n",
    "        xs.append(x)\n",
    "        ys.append(func(x))\n",
    "    return xs, ys    \n",
    "\n",
    "\n",
    "xs = np.arange(-2, 3, 0.1)\n",
    "ys = map(func, xs)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(xs, ys, alpha=0.5, ls='dashed')\n",
    "\n",
    "# Start gradient descent at x = -1.5\n",
    "xs2, ys2 = iterate(-1.5, n=100)\n",
    "plt.scatter(xs2, ys2, c='r', s=100)\n",
    "\n",
    "# Start gradient descent at x = 2; where does it go?\n",
    "xs2, ys2 = iterate(2, n=100)\n",
    "plt.scatter(xs2, ys2, c='y', s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot your results\n",
    "\n",
    "Again make sure that you record any valuable information. For example, in the tax scenario, did you find the sales from the first three months of the year to be a good predictor of the total sales for the year? Plot the predictions versus the true values and discuss the successes and limitations of your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the data and the best fit line\n",
    "## The data\n",
    "plt.scatter(X, y)\n",
    "## The line / model\n",
    "plt.plot(X, predictions)\n",
    "\n",
    "plt.show()\n",
    "print \"r^2:\", model.score(X, y)\n",
    "print \"RMSE:\", mean_squared_error(ys, predictions)\n",
    "print \"MAE:\", mean_absolute_error(ys, predictions)\n",
    "print \"Coefficients:\", model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Present the Results\n",
    "\n",
    "Present your conclusions and results. If you have more than one interesting model feel free to include more than one along with a discussion. Use your work in this notebook to prepare your write-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
